<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Q&amp;A with LlamaIndex &mdash; NVIDIA Generative AI Examples 0.5.0 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" type="text/css" />
      <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="../_static/omni-style.css" type="text/css" />
      <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/clipboard.min.js"></script>
        <script src="../_static/copybutton.js"></script>
        <script src="../_static/version.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Advanced Q&amp;A with LlamaIndex" href="04_llamaindex_hier_node_parser.html" />
    <link rel="prev" title="Q&amp;A with LangChain" href="02_langchain_simple.html" />
 


</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >


<a href="../index.html">
  <img src="../_static/nvidia-logo-white.png" class="logo" alt="Logo"/>
</a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">RAG Pipelines for Developers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../index.html">About the RAG Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../support-matrix.html">Support Matrix</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ai-foundation-models.html">AI Foundation Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../local-gpu.html">Local GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multi-gpu.html">Multi-GPU for Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../query-decomposition.html">Query Decomposition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quantized-llm-model.html">Quantized Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../structured-data.html">Structured Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multimodal-data.html">Multimodal Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multi-turn.html">Multi-turn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../using-sample-web-application.html">Sample Chat Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="../vector-database.html">Alternative Vector Database</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../evaluation.html">Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../observability.html">Observability</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Jupyter Notebooks</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="00-llm-non-streaming-nemotron.html">Basics: Prompt, Client, and Responses</a></li>
<li class="toctree-l1"><a class="reference internal" href="01-llm-streaming-client.html">LLM Streaming Client</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_langchain_simple.html">Q&amp;A with LangChain</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Q&amp;A with LlamaIndex</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_llamaindex_hier_node_parser.html">Advanced Q&amp;A with LlamaIndex</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_dataloader.html">Press Release Chat Bot</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_Option%281%29_NVIDIA_AI_endpoint_simple.html">NVIDIA AI Endpoints with LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_Option%282%29_minimalistic_RAG_with_langchain_local_HF_LLM.html">LangChain with Local Llama 2 Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_Option%281%29_llama_index_with_NVIDIA_AI_endpoint.html">NVIDIA AI Endpoints, LlamaIndex, and LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_Option%282%29_llama_index_with_HF_local_LLM.html">HF Checkpoints with LlamaIndex and LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="09_Agent_use_tools_leveraging_NVIDIA_AI_endpoints.html">Multimodal Models from NVIDIA AI Endpoints with LangChain Agent</a></li>
<li class="toctree-l1"><a class="reference internal" href="10_RAG_for_HTML_docs_with_Langchain_NVIDIA_AI_Endpoints.html">Build a RAG chain by generating embeddings for NVIDIA Triton documentation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Software Components</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../architecture.html">Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../llm-inference-server.html">NeMo Framework Inference Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="../frontend.html">RAG Playground Web Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="../jupyter-server.html">Jupyter Notebook Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chain-server.html">Chain Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="../configuration.html">Software Component Configuration</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">NVIDIA Generative AI Examples</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Q&amp;A with LlamaIndex</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="q-a-with-llamaindex">
<h1>Q&amp;A with LlamaIndex<a class="headerlink" href="#q-a-with-llamaindex" title="Permalink to this headline"></a></h1>
<p>This notebook demonstrates how to use <a class="reference external" href="https://docs.llamaindex.ai/en/stable/">LlamaIndex</a> to build a chatbot that references a custom knowledge base.</p>
<p>Suppose you have some text documents (PDF, blog, Notion pages, etc.) and want to ask questions related to the contents of those documents. LLMs, given their proficiency in understanding text, are a great tool for this.</p>
<div class="alert alert-block alert-info">
<p>⚠️ The notebook before this one, <code class="docutils literal notranslate"><span class="pre">02_langchain_index_simple.ipynb</span></code>, contains the same functionality as this notebook but uses some LangChain components instead of LlamaIndex components.</p>
<p>Concepts that are used in this notebook are explained in-depth in the previous notebook. If you are new to retrieval augmented generation, it is recommended to go through the previous notebook before this one.</p>
<p>Ultimately, we recommend reading about LangChain vs. LlamaIndex and picking the software/components of the software that makes the most sense to you. This is discussed a bit further below.</p>
</div>
<section id="llamaindex">
<h2><a class="reference external" href="https://docs.llamaindex.ai/en/stable/">LlamaIndex</a><a class="headerlink" href="#llamaindex" title="Permalink to this headline"></a></h2>
<p><a class="reference external" href="https://docs.llamaindex.ai/en/stable/"><strong>LlamaIndex</strong></a> is a data framework for LLM applications to ingest, structure, and access private or domain-specific data. Since LLMs are both only trained up to a fixed point in time and do not contain knowledge that is proprietary to an Enterprise, they can’t answer questions about new or proprietary knowledge. LlamaIndex helps solve this problem by providing data connectors to ingest data, indices to structure data for storage, and engines to communicate with data.</p>
</section>
<section id="llamaindex-or-langchain">
<h2><a class="reference external" href="https://docs.llamaindex.ai/en/stable/">LlamaIndex</a> or <a class="reference external" href="https://python.langchain.com/docs/get_started/introduction">LangChain</a>?<a class="headerlink" href="#llamaindex-or-langchain" title="Permalink to this headline"></a></h2>
<p>It’s recommended to read more about the unique strengths of both LlamaIndex and LangChain. At a high level, LangChain is a more general framework for building applications with LLMs. LangChain is (currently) more mature when it comes to multi-step chains and some other chat functionality such as conversational memory. LlamaIndex has plenty of overlap with LangChain, but is particularly strong for loading data from a wide variety of sources and indexing/querying tasks.</p>
<p>Since LlamaIndex can be used <em>with</em> LangChain, the frameworks’ unique capabilities can be leveraged together; the combination of the two is demonstrated in this notebook.</p>
</section>
<section id="step-1-integrate-tensorrt-llm-to-langchain-and-llamaindex">
<h2>Step 1: Integrate TensorRT-LLM to LangChain <em>and</em> LlamaIndex<a class="headerlink" href="#step-1-integrate-tensorrt-llm-to-langchain-and-llamaindex" title="Permalink to this headline"></a></h2>
<section id="customized-langchain-llm-in-llamaindex">
<h3>Customized LangChain LLM in LlamaIndex<a class="headerlink" href="#customized-langchain-llm-in-llamaindex" title="Permalink to this headline"></a></h3>
<p>Langchain allows you to create custom wrappers for your LLM in case you want to use your own LLM or a different wrapper than the one that is supported in LangChain. Since we are using LlamaIndex, we have written a custom langchain wrapper compatible with LlamaIndex.</p>
<p>We can easily take a custom LLM that has been wrapped for LangChain and plug it into <a class="reference external" href="https://docs.llamaindex.ai/en/stable/understanding/using_llms/using_llms.html#using-llms">LlamaIndex as an LLM</a>! We use the <a class="reference external" href="https://docs.llamaindex.ai/en/v0.9.48/api_reference/llms/langchain.html">LlamaIndex LangChainLLM library</a> so the LangChain LLM can be used in LlamaIndex.</p>
<div class="alert alert-block alert-warning">
<p><b>WARNING!</b> Be sure to replace <code class="docutils literal notranslate"><span class="pre">server_url</span></code> with the address and port that Triton is running on.</p>
</div>
<p>Use the address and port that the Triton is available on; for example <code class="docutils literal notranslate"><span class="pre">localhost:8001</span></code>. <strong>If you are running this notebook as part of the generative ai workflow, your can use the existing url.</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">triton_trt_llm</span> <span class="kn">import</span> <span class="n">TensorRTLLM</span>
<span class="kn">from</span> <span class="nn">llama_index.llms</span> <span class="kn">import</span> <span class="n">LangChainLLM</span>
<span class="n">trtllm</span> <span class="o">=</span><span class="n">TensorRTLLM</span><span class="p">(</span><span class="n">server_url</span> <span class="o">=</span><span class="s2">&quot;llm:8001&quot;</span><span class="p">,</span> <span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;ensemble&quot;</span><span class="p">,</span> <span class="n">tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">LangChainLLM</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">trtllm</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="step-2-create-a-prompt-template">
<h2>Step 2: Create a Prompt Template<a class="headerlink" href="#step-2-create-a-prompt-template" title="Permalink to this headline"></a></h2>
<p>A <a class="reference external" href="https://docs.llamaindex.ai/en/stable/module_guides/models/prompts.html"><strong>prompt template</strong></a> is a common paradigm in LLM development.</p>
<p>They are a pre-defined set of instructions provided to the LLM and guide the output produced by the model. They can contain few shot examples and guidance and are a quick way to engineer the responses from the LLM. Llama 2 accepts the <a class="reference external" href="https://huggingface.co/blog/llama2#how-to-prompt-llama-2">prompt format</a> shown in <code class="docutils literal notranslate"><span class="pre">LLAMA_PROMPT_TEMPLATE</span></code>, which we manipulate to be constructed with:</p>
<ul class="simple">
<li><p>The system prompt</p></li>
<li><p>The context</p></li>
<li><p>The user’s question</p></li>
</ul>
<p>Much like LangChain’s abstraction of prompts, LlamaIndex has similar abstractions for you to create prompts.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">llama_index</span> <span class="kn">import</span> <span class="n">Prompt</span>

<span class="n">LLAMA_PROMPT_TEMPLATE</span> <span class="o">=</span> <span class="p">(</span>
 <span class="s2">&quot;&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;&quot;</span>
 <span class="s2">&quot;Use the following context to answer the user&#39;s question. If you don&#39;t know the answer, just say that you don&#39;t know, don&#39;t try to make up an answer.&quot;</span>
 <span class="s2">&quot;&lt;&lt;/SYS&gt;&gt;&quot;</span>
 <span class="s2">&quot;&lt;s&gt;[INST] Context: </span><span class="si">{context_str}</span><span class="s2"> Question: </span><span class="si">{query_str}</span><span class="s2"> Only return the helpful answer below and nothing else. Helpful answer:[/INST]&quot;</span>
<span class="p">)</span>

<span class="n">qa_template</span> <span class="o">=</span> <span class="n">Prompt</span><span class="p">(</span><span class="n">LLAMA_PROMPT_TEMPLATE</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="step-3-load-documents">
<h2>Step 3: Load Documents<a class="headerlink" href="#step-3-load-documents" title="Permalink to this headline"></a></h2>
<div>
<img src="./imgs/llama_hub.png" width="500"/>
</div>
<p>LlamaIndex provides <a class="reference external" href="https://docs.llamaindex.ai/en/stable/module_guides/loading/connector/root.html#data-connectors-llamahub"><strong>data loaders</strong></a> through Llama Hub.
These allow for custom data sources to be connected to your LLM using integrations.
For example, integrations are available to load documents from
Jira,
Outlook Calendar,
Slack,
Trello, and many other applications.</p>
<p>At the core of each data loader is a <code class="docutils literal notranslate"><span class="pre">download_loader</span></code> function which downloads the loader file into a module that you can use in your application. Once the loader is downloaded, data is ingested through the loader. The output of this ingestion is data formatted as a LlamaIndex <a class="reference external" href="https://docs.llamaindex.ai/en/stable/module_guides/loading/documents_and_nodes/root.html#documents-nodes"><strong>Document</strong></a> (text and metadata).</p>
<p>Similar to the previous notebook with LangChain, an <a class="reference external" href="https://llamahub.ai/l/readers/llama-index-readers-file"><code class="docutils literal notranslate"><span class="pre">UnstructuredReader</span></code></a> is used in this example. However, this time it’s from from <a class="reference external" href="https://llamahub.ai/">Llama Hub</a> (LlamaIndex). Again, we load a research paper about Llama2 from Meta.</p>
<p><a class="reference external" href="https://python.langchain.com/docs/integrations/document_loaders">Here</a> are some of the other document loaders available from LangChain.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span><span class="w"> </span>wget<span class="w"> </span>-O<span class="w"> </span><span class="s2">&quot;llama2_paper.pdf&quot;</span><span class="w"> </span>-nc<span class="w"> </span>--user-agent<span class="o">=</span><span class="s2">&quot;Mozilla&quot;</span><span class="w"> </span>https://arxiv.org/pdf/2307.09288.pdf
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>File ‘llama2_paper.pdf’ already there; not retrieving.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">llama_hub.file.unstructured.base</span> <span class="kn">import</span> <span class="n">UnstructuredReader</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="n">loader</span> <span class="o">=</span> <span class="n">UnstructuredReader</span><span class="p">()</span>
<span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">documents</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">load_data</span><span class="p">(</span><span class="n">file</span><span class="o">=</span><span class="s2">&quot;llama2_paper.pdf&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;--- </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">start_time</span><span class="si">}</span><span class="s2"> seconds ---&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[nltk_data] Downloading package punkt to /root/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data] Downloading package averaged_perceptron_tagger to
[nltk_data]     /root/nltk_data...
[nltk_data]   Package averaged_perceptron_tagger is already up-to-
[nltk_data]       date!
</pre></div>
</div>
</div>
</div>
</section>
<section id="step-4-transform-documents-with-text-splitting-and-a-node-parser">
<h2>Step 4: Transform Documents with Text Splitting and a Node Parser<a class="headerlink" href="#step-4-transform-documents-with-text-splitting-and-a-node-parser" title="Permalink to this headline"></a></h2>
<section id="a-generate-embeddings">
<h3>a) Generate Embeddings<a class="headerlink" href="#a-generate-embeddings" title="Permalink to this headline"></a></h3>
<p>Once documents have been loaded, they are often transformed. One method of transformation is known as <strong>chunking</strong>, which breaks down large pieces of text, for example, a long document, into smaller segments. This technique is valuable because it helps <a class="reference external" href="https://www.pinecone.io/learn/chunking-strategies/">optimize the relevance of the content returned from the vector database</a>.</p>
<p>This is the same process as the previous notebook; again, we use a LangChain text splitter. In this example, we use a <a class="reference external" href="https://api.python.langchain.com/en/latest/sentence_transformers/langchain_text_splitters.sentence_transformers.SentenceTransformersTokenTextSplitter.html"><code class="docutils literal notranslate"><span class="pre">SentenceTransformersTokenTextSplitter</span></code></a>. The <code class="docutils literal notranslate"><span class="pre">SentenceTransformersTokenTextSplitter</span></code> is a specialized text splitter for use with the sentence-transformer models. The default behavior is to split the text into chunks that fit the token window of the sentence transformer model that you would like to use. This sentence transformer model is used to generate the embeddings from documents.</p>
<p>There are some nuanced complexities to text splitting since semantically related text, in theory, should be kept together.</p>
<p>To use the Langchain’s <code class="docutils literal notranslate"><span class="pre">SentenceTransformersTokenTextSplitter</span></code> with LlamaIndex we use the <a class="reference external" href="https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules.html#langchainnodeparser"><strong>Langchain node parser</strong></a> on top of the text splitter from LangChain. This is not required, but since LlamaIndex provides a <a class="reference external" href="https://docs.llamaindex.ai/en/stable/module_guides/loading/documents_and_nodes/root.html#documents-nodes"><strong>node structure</strong></a>, we choose to use this functionality to level up our storage of documents.</p>
<p><strong>Nodes</strong> represent chunks of source documents, but they also contain metadata and relationship information with other nodes and index structures. Since nodes provide these additional forms of hierarchy and connections across the data, they can help generate more accurate answers upon retrieval.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain.text_splitter</span> <span class="kn">import</span> <span class="n">SentenceTransformersTokenTextSplitter</span>
<span class="kn">from</span> <span class="nn">llama_index.node_parser</span> <span class="kn">import</span> <span class="n">LangchainNodeParser</span>


<span class="n">TEXT_SPLITTER_MODEL</span> <span class="o">=</span> <span class="s2">&quot;intfloat/e5-large-v2&quot;</span>
<span class="n">TEXT_SPLITTER_TOKENS_PER_CHUNK</span> <span class="o">=</span> <span class="mi">510</span>
<span class="n">TEXT_SPLITTER_CHUNCK_OVERLAP</span> <span class="o">=</span> <span class="mi">200</span>

<span class="n">text_splitter</span> <span class="o">=</span> <span class="n">SentenceTransformersTokenTextSplitter</span><span class="p">(</span>
    <span class="n">model_name</span><span class="o">=</span><span class="n">TEXT_SPLITTER_MODEL</span><span class="p">,</span>
    <span class="n">tokens_per_chunk</span><span class="o">=</span><span class="n">TEXT_SPLITTER_TOKENS_PER_CHUNK</span><span class="p">,</span>
    <span class="n">chunk_overlap</span><span class="o">=</span><span class="n">TEXT_SPLITTER_CHUNCK_OVERLAP</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">node_parser</span> <span class="o">=</span> <span class="n">LangchainNodeParser</span><span class="p">(</span><span class="n">text_splitter</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm
</pre></div>
</div>
</div>
</div>
<p>Additionally, we use a LlamaIndex <a class="reference external" href="https://docs.llamaindex.ai/en/stable/api_reference/service_context/prompt_helper.html"><code class="docutils literal notranslate"><span class="pre">PromptHelper</span></code></a> to help deal with LLM context window token limitations. It calculates available context size to the LLM by taking the initial context token length and subtracting out reserved token space for the prompt template and output. It provides a utility for re-packing text chunks from the index to maximally use the context window to minimize requests sent to the LLM.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">context_window</span></code>: context window for the LLM – the context length for Llama2 is 4k tokens</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">num_ouptut</span></code>: number of output tokens for the LLM</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">chunk_overlap_ratio</span></code>: chunk overlap as a ratio to chunk size</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">chunk_size_limit</span></code>: maximum chunk size to use</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">llama_index</span> <span class="kn">import</span> <span class="n">PromptHelper</span>

<span class="n">prompt_helper</span> <span class="o">=</span> <span class="n">PromptHelper</span><span class="p">(</span>
  <span class="n">context_window</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span>
  <span class="n">num_output</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
  <span class="n">chunk_overlap_ratio</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
  <span class="n">chunk_size_limit</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="step-5-generate-and-store-embeddings">
<h2>Step 5: Generate and Store Embeddings<a class="headerlink" href="#step-5-generate-and-store-embeddings" title="Permalink to this headline"></a></h2>
<section id="id1">
<h3>a) Generate Embeddings<a class="headerlink" href="#id1" title="Permalink to this headline"></a></h3>
<p><a class="reference external" href="https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings.html#embeddings">Embeddings</a> for documents are created by vectorizing the document text; this vectorization captures the semantic meaning of the text. This allows you to quickly and efficiently find other pieces of text that are similar.</p>
<p>When a user sends in their query, the query is also embedded using the same embedding model that was used to embed the documents. As explained earlier, this allows us to find similar (relevant) documents to the user’s query.</p>
<p>Like other sections in this notebook, we can easily take a LangChain embedding object and use with LlamaIndex. We use the <a class="reference external" href="https://docs.llamaindex.ai/en/stable/api_reference/service_context/embeddings.html#langchainembedding">LangchainEmbedding library</a>, which acts as a wrapper around Langchain’s embedding models.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain.embeddings</span> <span class="kn">import</span> <span class="n">HuggingFaceEmbeddings</span>
<span class="kn">from</span> <span class="nn">llama_index.embeddings</span> <span class="kn">import</span> <span class="n">LangchainEmbedding</span>

<span class="c1">#Running the model on CPU as we want to conserve gpu memory.</span>
<span class="c1">#In the production deployment (API server shown as part of the 5th notebook we run the model on GPU)</span>
<span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;intfloat/e5-large-v2&quot;</span>
<span class="n">model_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;device&quot;</span><span class="p">:</span> <span class="s2">&quot;cpu&quot;</span><span class="p">}</span>
<span class="n">encode_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;normalize_embeddings&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">}</span>
<span class="n">hf_embeddings</span> <span class="o">=</span> <span class="n">HuggingFaceEmbeddings</span><span class="p">(</span>
    <span class="n">model_name</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span>
    <span class="n">model_kwargs</span><span class="o">=</span><span class="n">model_kwargs</span><span class="p">,</span>
    <span class="n">encode_kwargs</span><span class="o">=</span><span class="n">encode_kwargs</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># Load in a specific embedding model</span>
<span class="n">embed_model</span> <span class="o">=</span> <span class="n">LangchainEmbedding</span><span class="p">(</span><span class="n">hf_embeddings</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="b-store-embeddings">
<h3>b) Store Embeddings<a class="headerlink" href="#b-store-embeddings" title="Permalink to this headline"></a></h3>
<p>LlamaIndex provides a supporting module, <a class="reference external" href="https://docs.llamaindex.ai/en/v0.10.19/api_reference/service_context.html"><code class="docutils literal notranslate"><span class="pre">ServiceContext</span></code></a>, to bundle commonly used resources during the indexing and querying stage. In this example, we bundle resources we’ve built: the LLM, the embedding model, the node parser, and the prompt helper.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">llama_index</span> <span class="kn">import</span> <span class="n">ServiceContext</span>
<span class="n">service_context</span> <span class="o">=</span> <span class="n">ServiceContext</span><span class="o">.</span><span class="n">from_defaults</span><span class="p">(</span>
  <span class="n">llm</span><span class="o">=</span><span class="n">llm</span><span class="p">,</span>
  <span class="n">embed_model</span><span class="o">=</span><span class="n">embed_model</span><span class="p">,</span>
  <span class="n">node_parser</span><span class="o">=</span><span class="n">node_parser</span><span class="p">,</span>
  <span class="n">prompt_helper</span><span class="o">=</span><span class="n">prompt_helper</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Set the service context globally, to avoid passing it to every llm call/</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">llama_index</span> <span class="kn">import</span> <span class="n">set_global_service_context</span>
<span class="n">set_global_service_context</span><span class="p">(</span><span class="n">service_context</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="alert alert-block alert-info">
<p>⚠️ in the deployment of this workflow, <a class="reference external" href="https://milvus.io/">Milvus</a> is running as a vector database microservice.</p>
</div><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">llama_index</span> <span class="kn">import</span> <span class="n">VectorStoreIndex</span>
<span class="kn">from</span> <span class="nn">llama_index.storage.storage_context</span> <span class="kn">import</span> <span class="n">StorageContext</span>
<span class="kn">from</span> <span class="nn">llama_index.vector_stores</span> <span class="kn">import</span> <span class="n">MilvusVectorStore</span>

<span class="n">vector_store</span> <span class="o">=</span> <span class="n">MilvusVectorStore</span><span class="p">(</span><span class="n">uri</span><span class="o">=</span><span class="s2">&quot;http://milvus:19530&quot;</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">overwrite</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">storage_context</span> <span class="o">=</span> <span class="n">StorageContext</span><span class="o">.</span><span class="n">from_defaults</span><span class="p">(</span><span class="n">vector_store</span><span class="o">=</span><span class="n">vector_store</span><span class="p">)</span>
<span class="n">index</span> <span class="o">=</span> <span class="n">VectorStoreIndex</span><span class="o">.</span><span class="n">from_vector_store</span><span class="p">(</span><span class="n">vector_store</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s load the documents into the vector database index</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>
<span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">nodes</span> <span class="o">=</span> <span class="n">node_parser</span><span class="o">.</span><span class="n">get_nodes_from_documents</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
<span class="n">index</span><span class="o">.</span><span class="n">insert_nodes</span><span class="p">(</span><span class="n">nodes</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;--- </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">start_time</span><span class="si">}</span><span class="s2"> seconds ---&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="step-6-build-the-query-engine-and-stream-response">
<h2>Step 6: Build the Query Engine and Stream Response<a class="headerlink" href="#step-6-build-the-query-engine-and-stream-response" title="Permalink to this headline"></a></h2>
<section id="a-build-the-query-engine">
<h3>a) Build the Query Engine<a class="headerlink" href="#a-build-the-query-engine" title="Permalink to this headline"></a></h3>
<p>A query engine is an object that takes in a query and returns a response. Each vector index has a default corresponding query engine; for example, the default query engine for a vector index performs a standard top-k retrieval over the vector store.</p>
<p>A query engine contains the following components:</p>
<ul class="simple">
<li><p>Retriever</p></li>
<li><p>Node PostProcessor</p></li>
<li><p>Response Synthesizer</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">query_engine</span> <span class="o">=</span> <span class="n">index</span><span class="o">.</span><span class="n">as_query_engine</span><span class="p">(</span><span class="n">text_qa_template</span><span class="o">=</span><span class="n">qa_template</span><span class="p">,</span> <span class="n">streaming</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="b-stream-a-response-from-the-query-engine">
<h3>b) Stream a Response from the Query Engine<a class="headerlink" href="#b-stream-a-response-from-the-query-engine" title="Permalink to this headline"></a></h3>
<p>Lastly, we pass the query engine a user’s question and stream the response.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>

<span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">query_engine</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s2">&quot;what is the context length of llama2?&quot;</span><span class="p">)</span>
<span class="n">response</span><span class="o">.</span><span class="n">print_response_stream</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">--- </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">start_time</span><span class="si">}</span><span class="s2"> seconds ---&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="02_langchain_simple.html" class="btn btn-neutral float-left" title="Q&amp;A with LangChain" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="04_llamaindex_hier_node_parser.html" class="btn btn-neutral float-right" title="Advanced Q&amp;A with LlamaIndex" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023-2024, NVIDIA.</p>
  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 



</body>
</html>