<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Advanced Q&amp;A with LlamaIndex &mdash; NVIDIA Generative AI Examples 0.5.0 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" type="text/css" />
      <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="../_static/omni-style.css" type="text/css" />
      <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/clipboard.min.js"></script>
        <script src="../_static/copybutton.js"></script>
        <script src="../_static/version.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Press Release Chat Bot" href="05_dataloader.html" />
    <link rel="prev" title="Q&amp;A with LlamaIndex" href="03_llama_index_simple.html" />
 


</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >


<a href="../index.html">
  <img src="../_static/nvidia-logo-white.png" class="logo" alt="Logo"/>
</a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">RAG Pipelines for Developers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../index.html">About the RAG Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../support-matrix.html">Support Matrix</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ai-foundation-models.html">AI Foundation Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../local-gpu.html">Local GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multi-gpu.html">Multi-GPU for Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../query-decomposition.html">Query Decomposition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quantized-llm-model.html">Quantized Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../structured-data.html">Structured Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multimodal-data.html">Multimodal Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multi-turn.html">Multi-turn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../using-sample-web-application.html">Sample Chat Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="../vector-database.html">Alternative Vector Database</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../evaluation.html">Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../observability.html">Observability</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Jupyter Notebooks</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="00-llm-non-streaming-nemotron.html">Basics: Prompt, Client, and Responses</a></li>
<li class="toctree-l1"><a class="reference internal" href="01-llm-streaming-client.html">LLM Streaming Client</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_langchain_simple.html">Q&amp;A with LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_llama_index_simple.html">Q&amp;A with LlamaIndex</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Advanced Q&amp;A with LlamaIndex</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_dataloader.html">Press Release Chat Bot</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_Option%281%29_NVIDIA_AI_endpoint_simple.html">NVIDIA AI Endpoints with LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_Option%282%29_minimalistic_RAG_with_langchain_local_HF_LLM.html">LangChain with Local Llama 2 Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_Option%281%29_llama_index_with_NVIDIA_AI_endpoint.html">NVIDIA AI Endpoints, LlamaIndex, and LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_Option%282%29_llama_index_with_HF_local_LLM.html">HF Checkpoints with LlamaIndex and LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="09_Agent_use_tools_leveraging_NVIDIA_AI_endpoints.html">Multimodal Models from NVIDIA AI Endpoints with LangChain Agent</a></li>
<li class="toctree-l1"><a class="reference internal" href="10_RAG_for_HTML_docs_with_Langchain_NVIDIA_AI_Endpoints.html">Build a RAG chain by generating embeddings for NVIDIA Triton documentation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Software Components</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../architecture.html">Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../llm-inference-server.html">NeMo Framework Inference Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="../frontend.html">RAG Playground Web Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="../jupyter-server.html">Jupyter Notebook Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chain-server.html">Chain Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="../configuration.html">Software Component Configuration</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">NVIDIA Generative AI Examples</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Advanced Q&amp;A with LlamaIndex</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="advanced-q-a-with-llamaindex">
<h1>Advanced Q&amp;A with LlamaIndex<a class="headerlink" href="#advanced-q-a-with-llamaindex" title="Permalink to this headline"></a></h1>
<p>This notebook demonstrates how to use <a class="reference external" href="https://docs.llamaindex.ai/en/stable/">LlamaIndex</a> to build a more complex retrieval for a chatbot.</p>
<p>The retrieval method shown in this notebook works well for code documentation; it retrieves more contiguous document blocks that preserve both code snippets and explanations of code.</p>
<div class="alert alert-block alert-info">
<p>⚠️ There are many node parsing and retrieval techniques supported in LlamaIndex and this notebook just shows how two of these techniques, <a class="reference external" href="https://docs.llamaindex.ai/en/stable/api_reference/service_context/node_parser.html">HierarchialNodeParser</a> and <a class="reference external" href="https://docs.llamaindex.ai/en/stable/examples/retrievers/auto_merging_retriever.html">AutoMergingRetriever</a>, can be useful for chatting with code documentation.</p>
</div>
<p>In this demo, we’ll use the <a class="reference external" href="https://github.com/run-llama/llama_docs_bot/tree/main"><code class="docutils literal notranslate"><span class="pre">llama_docs_bot</span></code></a> GitHub repository as our sample documentation to query. This repository contains the content for a development series with LlamaIndex covering the following topics:</p>
<ul class="simple">
<li><p>LLMs</p></li>
<li><p>Nodes and documents</p></li>
<li><p>Evaluation</p></li>
<li><p>Embeddings</p></li>
<li><p>Retrieval</p></li>
</ul>
<section id="step-1-prerequisite-setup">
<h2>Step 1: Prerequisite Setup<a class="headerlink" href="#step-1-prerequisite-setup" title="Permalink to this headline"></a></h2>
<p>By now you should be familiar with these steps:</p>
<ol class="arabic simple">
<li><p>Create an LLM client.</p></li>
<li><p>Set the prompt template for the LLM.</p></li>
<li><p>Download embeddings.</p></li>
<li><p>Set the service context.</p></li>
<li><p>Split the text</p></li>
</ol>
<div class="alert alert-block alert-warning">
<p><b>WARNING!</b> Be sure to replace <code class="docutils literal notranslate"><span class="pre">server_url</span></code> with the address and port that Triton is running on.</p>
</div>
<p>Use the address and port that the Triton is available on; for example <code class="docutils literal notranslate"><span class="pre">localhost:8001</span></code>. **If you are running this notebook as part of the generative ai workflow, you can use the existing url.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">triton_trt_llm</span> <span class="kn">import</span> <span class="n">TensorRTLLM</span>
<span class="kn">from</span> <span class="nn">llama_index.llms</span> <span class="kn">import</span> <span class="n">LangChainLLM</span>
<span class="n">trtllm</span> <span class="o">=</span><span class="n">TensorRTLLM</span><span class="p">(</span><span class="n">server_url</span> <span class="o">=</span><span class="s2">&quot;llm:8001&quot;</span><span class="p">,</span> <span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;ensemble&quot;</span><span class="p">,</span> <span class="n">tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">LangChainLLM</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">trtllm</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">llama_index</span> <span class="kn">import</span> <span class="n">Prompt</span>

<span class="n">LLAMA_PROMPT_TEMPLATE</span> <span class="o">=</span> <span class="p">(</span>
 <span class="s2">&quot;&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;&quot;</span>
 <span class="s2">&quot;Use the following context to answer the user&#39;s question. If you don&#39;t know the answer, just say that you don&#39;t know, don&#39;t try to make up an answer.&quot;</span>
 <span class="s2">&quot;&lt;&lt;/SYS&gt;&gt;&quot;</span>
 <span class="s2">&quot;&lt;s&gt;[INST] Context: </span><span class="si">{context_str}</span><span class="s2"> Question: </span><span class="si">{query_str}</span><span class="s2"> Only return the helpful answer below and nothing else. Helpful answer:[/INST]&quot;</span>
<span class="p">)</span>

<span class="n">qa_template</span> <span class="o">=</span> <span class="n">Prompt</span><span class="p">(</span><span class="n">LLAMA_PROMPT_TEMPLATE</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain.embeddings</span> <span class="kn">import</span> <span class="n">HuggingFaceEmbeddings</span>
<span class="kn">from</span> <span class="nn">llama_index.embeddings</span> <span class="kn">import</span> <span class="n">LangchainEmbedding</span>
<span class="kn">from</span> <span class="nn">llama_index</span> <span class="kn">import</span> <span class="n">ServiceContext</span><span class="p">,</span> <span class="n">set_global_service_context</span>

<span class="n">model_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;device&quot;</span><span class="p">:</span> <span class="s2">&quot;cpu&quot;</span><span class="p">}</span>
<span class="n">encode_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;normalize_embeddings&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">}</span>
<span class="n">hf_embeddings</span> <span class="o">=</span> <span class="n">HuggingFaceEmbeddings</span><span class="p">(</span>
    <span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;intfloat/e5-large-v2&quot;</span><span class="p">,</span>
    <span class="n">model_kwargs</span><span class="o">=</span><span class="n">model_kwargs</span><span class="p">,</span>
    <span class="n">encode_kwargs</span><span class="o">=</span><span class="n">encode_kwargs</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># Load in a specific embedding model</span>
<span class="n">embed_model</span> <span class="o">=</span> <span class="n">LangchainEmbedding</span><span class="p">(</span><span class="n">hf_embeddings</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">service_context</span> <span class="o">=</span> <span class="n">ServiceContext</span><span class="o">.</span><span class="n">from_defaults</span><span class="p">(</span>
  <span class="n">llm</span><span class="o">=</span><span class="n">llm</span><span class="p">,</span>
  <span class="n">embed_model</span><span class="o">=</span><span class="n">embed_model</span>
<span class="p">)</span>
<span class="n">set_global_service_context</span><span class="p">(</span><span class="n">service_context</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>When splitting the text, we split it into a parent node of 1024 tokens and two children nodes of 510 tokens. Our leaf nodes’ maximum size is 512 tokens, so we need to make the largest leaves that can exist under 512 tokens.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">llama_index.text_splitter</span> <span class="kn">import</span> <span class="n">TokenTextSplitter</span>
<span class="n">text_splitter_ids</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;1024&quot;</span><span class="p">,</span> <span class="s2">&quot;510&quot;</span><span class="p">]</span>
<span class="n">text_splitter_map</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">ids</span> <span class="ow">in</span> <span class="n">text_splitter_ids</span><span class="p">:</span>
    <span class="n">text_splitter_map</span><span class="p">[</span><span class="n">ids</span><span class="p">]</span> <span class="o">=</span> <span class="n">TokenTextSplitter</span><span class="p">(</span>
        <span class="n">chunk_size</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">ids</span><span class="p">),</span>
        <span class="n">chunk_overlap</span><span class="o">=</span><span class="mi">200</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="step-2-clone-the-llama-docs-bot-repo">
<h2>Step 2: Clone the Llama Docs Bot Repo<a class="headerlink" href="#step-2-clone-the-llama-docs-bot-repo" title="Permalink to this headline"></a></h2>
<p>This repository will be our sample documentation that we chat with.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/run-llama/llama_docs_bot.git
</pre></div>
</div>
</div>
</div>
</section>
<section id="step-3-define-document-loading-and-node-parsing-function">
<h2>Step 3: Define Document Loading and Node Parsing Function<a class="headerlink" href="#step-3-define-document-loading-and-node-parsing-function" title="Permalink to this headline"></a></h2>
<p>Assuming hierarchical node parsing is set to true, this function:</p>
<ul>
<li><p>Parses each directory into a single giant document</p></li>
<li><p>Chunks the document into a hierarchy of nodes with a top-level chunk size (1024) and children chunks that are smaller (aka <strong>hierarchical node parsing</strong>)</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">      1024</span>
<span class="go">   /--------\</span>
<span class="go">1024//2     1024//2</span>
</pre></div>
</div>
</li>
</ul>
<section id="hierarchical-node-parser">
<h3>Hierarchical Node Parser<a class="headerlink" href="#hierarchical-node-parser" title="Permalink to this headline"></a></h3>
<p>The novel part of this step is using LlamaIndex’s <a class="reference external" href="https://docs.llamaindex.ai/en/stable/api/llama_index.core.node_parser.HierarchicalNodeParser.html#llama_index.core.node_parser.HierarchicalNodeParser"><strong>Hierarchical Node Parser</strong></a>. This parses nodes into several chunk sizes.</p>
<p>During retrieval, if a majority of chunks are retrieved that have the same parent chunk, the larger parent chunk is returned instead of the smaller chunks.</p>
</section>
<section id="simple-node-parser">
<h3>Simple Node Parser<a class="headerlink" href="#simple-node-parser" title="Permalink to this headline"></a></h3>
<p>If hierarchical parsing is false, a simple node structure is used and returned.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">llama_index</span> <span class="kn">import</span> <span class="n">SimpleDirectoryReader</span><span class="p">,</span> <span class="n">Document</span>
<span class="kn">from</span> <span class="nn">llama_index.node_parser</span> <span class="kn">import</span> <span class="n">HierarchicalNodeParser</span><span class="p">,</span> <span class="n">SimpleNodeParser</span><span class="p">,</span> <span class="n">get_leaf_nodes</span>
<span class="kn">from</span> <span class="nn">llama_index.schema</span> <span class="kn">import</span> <span class="n">MetadataMode</span>
<span class="kn">from</span> <span class="nn">llama_docs_bot.llama_docs_bot.markdown_docs_reader</span> <span class="kn">import</span> <span class="n">MarkdownDocsReader</span>

<span class="c1"># This function takes in a directory of files, puts them in a giant document, and parses and returns them as:</span>
<span class="c1"># - a hierarchical node structure if it&#39;s a hierarchical implementation</span>
<span class="c1"># - a simple node structure if it&#39;s a non-hierarchial implementation</span>
<span class="k">def</span> <span class="nf">load_markdown_docs</span><span class="p">(</span><span class="n">filepath</span><span class="p">,</span> <span class="n">hierarchical</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Load markdown docs from a directory, excluding all other file types.&quot;&quot;&quot;</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">SimpleDirectoryReader</span><span class="p">(</span>
        <span class="n">input_dir</span><span class="o">=</span><span class="n">filepath</span><span class="p">,</span>
        <span class="n">required_exts</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;.md&quot;</span><span class="p">],</span>
        <span class="n">file_extractor</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;.md&quot;</span><span class="p">:</span> <span class="n">MarkdownDocsReader</span><span class="p">()},</span>
        <span class="n">recursive</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>

    <span class="n">documents</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">hierarchical</span><span class="p">:</span>
        <span class="c1"># combine all documents into one</span>
        <span class="n">documents</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">Document</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                    <span class="n">document</span><span class="o">.</span><span class="n">get_content</span><span class="p">(</span><span class="n">metadata_mode</span><span class="o">=</span><span class="n">MetadataMode</span><span class="o">.</span><span class="n">ALL</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">document</span> <span class="ow">in</span> <span class="n">documents</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="p">]</span>

        <span class="c1"># chunk into 3 levels</span>
        <span class="c1"># majority means 2/3 are retrieved before using the parent</span>
        <span class="n">large_chunk_size</span> <span class="o">=</span> <span class="mi">1536</span>
        <span class="n">node_parser</span> <span class="o">=</span> <span class="n">HierarchicalNodeParser</span><span class="o">.</span><span class="n">from_defaults</span><span class="p">(</span><span class="n">node_parser_ids</span><span class="o">=</span><span class="n">text_splitter_ids</span><span class="p">,</span> <span class="n">node_parser_map</span><span class="o">=</span><span class="n">text_splitter_map</span><span class="p">)</span>

        <span class="n">nodes</span> <span class="o">=</span> <span class="n">node_parser</span><span class="o">.</span><span class="n">get_nodes_from_documents</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">nodes</span><span class="p">,</span> <span class="n">get_leaf_nodes</span><span class="p">(</span><span class="n">nodes</span><span class="p">)</span>
    <span class="c1">########## This is NOT a hierarchical parser for demonstration purposes later in the notebook ##########</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">node_parser</span> <span class="o">=</span> <span class="n">SimpleNodeParser</span><span class="o">.</span><span class="n">from_defaults</span><span class="p">()</span>
        <span class="n">nodes</span> <span class="o">=</span> <span class="n">node_parser</span><span class="o">.</span><span class="n">get_nodes_from_documents</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">nodes</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="step-4-load-and-parse-documents-with-node-parser">
<h2>Step 4: Load and Parse Documents with Node Parser<a class="headerlink" href="#step-4-load-and-parse-documents-with-node-parser" title="Permalink to this headline"></a></h2>
<p>First, we define all of the documentation directories we want to pull from.</p>
<p>Next, we load the documentation and store parent nodes in a <code class="docutils literal notranslate"><span class="pre">SimpleDocumentStore</span></code> and leaf nodes in a <code class="docutils literal notranslate"><span class="pre">VectorStoreIndex</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">docs_directories</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;./llama_docs_bot/docs/community&quot;</span><span class="p">:</span> <span class="s2">&quot;Useful for information on community integrations with other libraries, vector dbs, and frameworks.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;./llama_docs_bot/docs/core_modules/agent_modules&quot;</span><span class="p">:</span> <span class="s2">&quot;Useful for information on data agents and tools for data agents.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;./llama_docs_bot/docs/core_modules/data_modules&quot;</span><span class="p">:</span> <span class="s2">&quot;Useful for information on data, storage, indexing, and data processing modules.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;./llama_docs_bot/docs/core_modules/model_modules&quot;</span><span class="p">:</span> <span class="s2">&quot;Useful for information on LLMs, embedding models, and prompts.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;./llama_docs_bot/docs/core_modules/query_modules&quot;</span><span class="p">:</span> <span class="s2">&quot;Useful for information on various query engines and retrievers, and anything related to querying data.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;./llama_docs_bot/docs/core_modules/supporting_modules&quot;</span><span class="p">:</span> <span class="s2">&quot;Useful for information on supporting modules, like callbacks, evaluators, and other supporting modules.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;./llama_docs_bot/docs/getting_started&quot;</span><span class="p">:</span> <span class="s2">&quot;Useful for information on getting started with LlamaIndex.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;./llama_docs_bot/docs/development&quot;</span><span class="p">:</span> <span class="s2">&quot;Useful for information on contributing to LlamaIndex development.&quot;</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">llama_index</span> <span class="kn">import</span> <span class="n">VectorStoreIndex</span><span class="p">,</span><span class="n">StorageContext</span><span class="p">,</span> <span class="n">load_index_from_storage</span>
<span class="kn">from</span> <span class="nn">llama_index.query_engine</span> <span class="kn">import</span> <span class="n">RetrieverQueryEngine</span>

<span class="kn">from</span> <span class="nn">llama_index.tools</span> <span class="kn">import</span> <span class="n">QueryEngineTool</span><span class="p">,</span> <span class="n">ToolMetadata</span>
<span class="kn">from</span> <span class="nn">llama_index.storage.docstore</span> <span class="kn">import</span> <span class="n">SimpleDocumentStore</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="k">for</span> <span class="n">directory</span><span class="p">,</span> <span class="n">description</span> <span class="ow">in</span> <span class="n">docs_directories</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">nodes</span><span class="p">,</span> <span class="n">leaf_nodes</span> <span class="o">=</span> <span class="n">load_markdown_docs</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span> <span class="n">hierarchical</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">docstore</span> <span class="o">=</span> <span class="n">SimpleDocumentStore</span><span class="p">()</span>
    <span class="n">docstore</span><span class="o">.</span><span class="n">add_documents</span><span class="p">(</span><span class="n">nodes</span><span class="p">)</span>
    <span class="n">storage_context</span> <span class="o">=</span> <span class="n">StorageContext</span><span class="o">.</span><span class="n">from_defaults</span><span class="p">(</span><span class="n">docstore</span><span class="o">=</span><span class="n">docstore</span><span class="p">)</span>

    <span class="n">index</span> <span class="o">=</span> <span class="n">VectorStoreIndex</span><span class="p">(</span><span class="n">leaf_nodes</span><span class="p">,</span> <span class="n">storage_context</span><span class="o">=</span><span class="n">storage_context</span><span class="p">)</span>
    <span class="n">index</span><span class="o">.</span><span class="n">storage_context</span><span class="o">.</span><span class="n">persist</span><span class="p">(</span><span class="n">persist_dir</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;./data_</span><span class="si">{</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">directory</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;--- </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">start_time</span><span class="si">}</span><span class="s2"> seconds ---&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="step-5-define-custom-node-post-processor">
<h2>Step 5: Define Custom Node Post-Processor<a class="headerlink" href="#step-5-define-custom-node-post-processor" title="Permalink to this headline"></a></h2>
<p>A <a class="reference external" href="https://docs.llamaindex.ai/en/stable/module_guides/querying/node_postprocessors/node_postprocessors.html#node-postprocessor-modules"><strong>Node PostProcessor</strong></a> takes a list of retrieved nodes and transforms them (filtering, replacement, etc).</p>
<p>This custom node post-processor provides a simple approach to approximate token counts and returns the most nodes that fit within the token count (2500 tokens). Nodes are already sorted, so the most similar ones are returned first.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Optional</span>

<span class="kn">from</span> <span class="nn">llama_index.utils</span> <span class="kn">import</span> <span class="n">globals_helper</span><span class="p">,</span> <span class="n">get_tokenizer</span>
<span class="kn">from</span> <span class="nn">llama_index.schema</span> <span class="kn">import</span> <span class="n">MetadataMode</span>

<span class="k">class</span> <span class="nc">LimitRetrievedNodesLength</span><span class="p">:</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">limit</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2500</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span> <span class="ow">or</span> <span class="n">get_tokenizer</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">limit</span> <span class="o">=</span> <span class="n">limit</span>

    <span class="k">def</span> <span class="nf">postprocess_nodes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nodes</span><span class="p">,</span> <span class="n">query_bundle</span><span class="p">):</span>
        <span class="n">included_nodes</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">current_length</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">nodes</span><span class="p">:</span>
            <span class="n">current_length</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_tokenizer</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">get_content</span><span class="p">(</span><span class="n">metadata_mode</span><span class="o">=</span><span class="n">MetadataMode</span><span class="o">.</span><span class="n">LLM</span><span class="p">)))</span>
            <span class="k">if</span> <span class="n">current_length</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">limit</span><span class="p">:</span>
                <span class="k">break</span>
            <span class="n">included_nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">included_nodes</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="step-5-build-the-retriever-and-query-engine">
<h2>Step 5: Build the Retriever and Query Engine<a class="headerlink" href="#step-5-build-the-retriever-and-query-engine" title="Permalink to this headline"></a></h2>
<section id="automergingretriever">
<h3>AutoMergingRetriever<a class="headerlink" href="#automergingretriever" title="Permalink to this headline"></a></h3>
<p>The <a class="reference external" href="https://docs.llamaindex.ai/en/stable/examples/retrievers/auto_merging_retriever.html"><code class="docutils literal notranslate"><span class="pre">AutoMergingRetriever</span></code></a> takes in a set of leaf nodes and recursively merges subsets of leaf nodes that reference a parent node beyond a given threshold. This allows for a consolidation of potentially disparate, smaller contexts into a larger context that may help synthesize disparate information.</p>
</section>
<section id="query-engine">
<h3>Query Engine<a class="headerlink" href="#query-engine" title="Permalink to this headline"></a></h3>
<p>A query engine is an object that takes in a query and returns a response.</p>
<p>It may contain the following components:</p>
<ul class="simple">
<li><p><strong>Retriever</strong>: Given a query, retrieves relevant nodes.</p>
<ul>
<li><p>This example uses an <code class="docutils literal notranslate"><span class="pre">AutoMergingRetriever</span></code> if it’s a hierarchial implementation.
<em>This replaces the retrieved nodes with the larger parent chunk</em>.</p></li>
</ul>
</li>
<li><p><strong>Node PostProcessor</strong>: Takes a list of retrieved nodes and transforms them (filtering, replacement, etc.)</p>
<ul>
<li><p>This example uses a post-processor that filters the retrieved nodes to a limited length.</p></li>
</ul>
</li>
<li><p><strong>Response Synthesizer</strong>: Takes a list of relevant nodes and synthesizes a response with an LLM.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">llama_index.retrievers</span> <span class="kn">import</span> <span class="n">AutoMergingRetriever</span>
<span class="kn">from</span> <span class="nn">llama_index.query_engine</span> <span class="kn">import</span> <span class="n">RetrieverQueryEngine</span>

<span class="n">retriever</span> <span class="o">=</span> <span class="n">AutoMergingRetriever</span><span class="p">(</span>
        <span class="n">index</span><span class="o">.</span><span class="n">as_retriever</span><span class="p">(</span><span class="n">similarity_top_k</span><span class="o">=</span><span class="mi">12</span><span class="p">),</span>
        <span class="n">storage_context</span><span class="o">=</span><span class="n">storage_context</span>
    <span class="p">)</span>

<span class="n">query_engine</span> <span class="o">=</span> <span class="n">RetrieverQueryEngine</span><span class="o">.</span><span class="n">from_args</span><span class="p">(</span>
    <span class="n">retriever</span><span class="p">,</span>
    <span class="n">text_qa_template</span><span class="o">=</span><span class="n">qa_template</span><span class="p">,</span>
    <span class="n">node_postprocessors</span><span class="o">=</span><span class="p">[</span><span class="n">LimitRetrievedNodesLength</span><span class="p">(</span><span class="n">limit</span><span class="o">=</span><span class="mi">2500</span><span class="p">)],</span>
    <span class="n">streaming</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="step-6-stream-response">
<h2>Step 6: Stream Response<a class="headerlink" href="#step-6-stream-response" title="Permalink to this headline"></a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">query</span> <span class="o">=</span> <span class="s2">&quot;How do I setup a weaviate vector db? Give me a code sample please.&quot;</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>

<span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">query_engine</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
<span class="n">response</span><span class="o">.</span><span class="n">print_response_stream</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">--- </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">start_time</span><span class="si">}</span><span class="s2"> seconds ---&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>To clear out cached data run:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>rm<span class="w"> </span>-rf<span class="w"> </span>data_*
</pre></div>
</div>
</div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="03_llama_index_simple.html" class="btn btn-neutral float-left" title="Q&amp;A with LlamaIndex" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="05_dataloader.html" class="btn btn-neutral float-right" title="Press Release Chat Bot" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023-2024, NVIDIA.</p>
  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 



</body>
</html>