<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Q&amp;A with LangChain &mdash; NVIDIA Generative AI Examples 0.5.0 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" type="text/css" />
      <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="../_static/omni-style.css" type="text/css" />
      <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/clipboard.min.js"></script>
        <script src="../_static/copybutton.js"></script>
        <script src="../_static/version.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Q&amp;A with LlamaIndex" href="03_llama_index_simple.html" />
    <link rel="prev" title="LLM Streaming Client" href="01-llm-streaming-client.html" />
 


</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >


<a href="../index.html">
  <img src="../_static/nvidia-logo-white.png" class="logo" alt="Logo"/>
</a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">RAG Pipelines for Developers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../index.html">About the RAG Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../support-matrix.html">Support Matrix</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ai-foundation-models.html">AI Foundation Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../local-gpu.html">Local GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multi-gpu.html">Multi-GPU for Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../query-decomposition.html">Query Decomposition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quantized-llm-model.html">Quantized Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../structured-data.html">Structured Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multimodal-data.html">Multimodal Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multi-turn.html">Multi-turn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../using-sample-web-application.html">Sample Chat Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="../vector-database.html">Alternative Vector Database</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../evaluation.html">Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../observability.html">Observability</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Jupyter Notebooks</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="00-llm-non-streaming-nemotron.html">Basics: Prompt, Client, and Responses</a></li>
<li class="toctree-l1"><a class="reference internal" href="01-llm-streaming-client.html">LLM Streaming Client</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Q&amp;A with LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_llama_index_simple.html">Q&amp;A with LlamaIndex</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_llamaindex_hier_node_parser.html">Advanced Q&amp;A with LlamaIndex</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_dataloader.html">Press Release Chat Bot</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_Option%281%29_NVIDIA_AI_endpoint_simple.html">NVIDIA AI Endpoints with LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_Option%282%29_minimalistic_RAG_with_langchain_local_HF_LLM.html">LangChain with Local Llama 2 Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_Option%281%29_llama_index_with_NVIDIA_AI_endpoint.html">NVIDIA AI Endpoints, LlamaIndex, and LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_Option%282%29_llama_index_with_HF_local_LLM.html">HF Checkpoints with LlamaIndex and LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="09_Agent_use_tools_leveraging_NVIDIA_AI_endpoints.html">Multimodal Models from NVIDIA AI Endpoints with LangChain Agent</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Software Components</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../architecture.html">Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../llm-inference-server.html">NeMo Framework Inference Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="../frontend.html">RAG Playground Web Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="../jupyter-server.html">Jupyter Notebook Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chain-server.html">Chain Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="../configuration.html">Software Component Configuration</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">NVIDIA Generative AI Examples</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Q&amp;A with LangChain</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="q-a-with-langchain">
<h1>Q&amp;A with LangChain<a class="headerlink" href="#q-a-with-langchain" title="Permalink to this headline"></a></h1>
<p>This notebook demonstrates how to use LangChain to build a chatbot that references a custom knowledge-base.</p>
<p>Suppose you have some text documents (PDF, blog, Notion pages, etc.) and want to ask questions related to the contents of those documents. LLMs, given their proficiency in understanding text, are a great tool for this.</p>
<section id="langchain">
<h2><a class="reference external" href="https://python.langchain.com/docs/get_started/introduction">LangChain</a><a class="headerlink" href="#langchain" title="Permalink to this headline"></a></h2>
<p><a class="reference external" href="https://python.langchain.com/docs/get_started/introduction"><strong>LangChain</strong></a> provides a simple framework for connecting LLMs to your own data sources. Since LLMs are both only trained up to a fixed point in time and do not contain knowledge that is proprietary to an Enterprise, they can’t answer questions about new or proprietary knowledge. LangChain solves this problem.</p>
<div class="alert alert-block alert-info">
<p>⚠️ The notebook after this one, <code class="docutils literal notranslate"><span class="pre">03_llama_index_simple.ipynb</span></code>, contains the same functionality as this notebook but uses LlamaIndex instead of LangChain. Ultimately, we recommend reading about LangChain vs. LlamaIndex and picking the software/components of the software that makes the most sense to you.</p>
</div><p><img alt="data_connection" src="../_images/data_connection_langchain.jpeg" /></p>
</section>
<section id="step-1-integrate-tensorrt-llm-to-langchain-connector">
<h2>Step 1: Integrate TensorRT-LLM to LangChain <a class="reference external" href="https://docs.llamaindex.ai/en/stable/examples/llm/nvidia_tensorrt.html"><em>(Connector)</em></a><a class="headerlink" href="#step-1-integrate-tensorrt-llm-to-langchain-connector" title="Permalink to this headline"></a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain_nvidia_trt.llms</span> <span class="kn">import</span> <span class="n">TritonTensorRTLLM</span>

<span class="c1"># Connect to the TRT-LLM Llama-2 model running on the Triton server at the url below</span>
<span class="c1"># Replace &quot;llm&quot; with the url of the system where llama2 is hosted</span>
<span class="n">triton_url</span> <span class="o">=</span> <span class="s2">&quot;llm:8001&quot;</span>
<span class="n">pload</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;tokens&#39;</span><span class="p">:</span><span class="mi">500</span><span class="p">,</span>
            <span class="s1">&#39;server_url&#39;</span><span class="p">:</span> <span class="n">triton_url</span><span class="p">,</span>
            <span class="s1">&#39;model_name&#39;</span><span class="p">:</span> <span class="s2">&quot;ensemble&quot;</span>
<span class="p">}</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">TritonTensorRTLLM</span><span class="p">(</span><span class="o">**</span><span class="n">pload</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="note-follow-this-step-for-nemotron-models">
<h3>Note: Follow this step for nemotron models<a class="headerlink" href="#note-follow-this-step-for-nemotron-models" title="Permalink to this headline"></a></h3>
<ol class="arabic simple">
<li><p>In case you have deployed a trt-llm optimized nemotron model following steps <span class="xref myst">here</span>, execute the cell below by uncommenting the lines. Here we use a custom wrapper for talking with the model server.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># from triton_trt_llm import TensorRTLLM</span>
<span class="c1"># llm = TensorRTLLM(server_url =&quot;llm:8000&quot;, model_name=&quot;ensemble&quot;, tokens=500, streaming=False)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="step-2-create-a-prompt-template-model-i-o">
<h2>Step 2: Create a Prompt Template <a class="reference external" href="https://python.langchain.com/docs/modules/model_io/"><em>(Model I/O)</em></a><a class="headerlink" href="#step-2-create-a-prompt-template-model-i-o" title="Permalink to this headline"></a></h2>
<p>A <a class="reference external" href="https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/"><strong>prompt template</strong></a> is a common paradigm in LLM development.</p>
<p>They are a pre-defined set of instructions provided to the LLM and guide the output produced by the model. They can contain few shot examples and guidance and are a quick way to engineer the responses from the LLM. Llama 2 accepts the <a class="reference external" href="https://huggingface.co/blog/llama2#how-to-prompt-llama-2">prompt format</a> shown in <code class="docutils literal notranslate"><span class="pre">LLAMA_PROMPT_TEMPLATE</span></code>, which we manipulate to be constructed with:</p>
<ul class="simple">
<li><p>The system prompt</p></li>
<li><p>The context</p></li>
<li><p>The user’s question
Langchain allows you to <a class="reference external" href="https://python.langchain.com/docs/modules/model_io/models/llms/custom_llm">create custom wrappers for your LLM</a> in case you want to use your own LLM or a different wrapper than the one that is supported in LangChain. Since we are using a custom Llama2 model hosted on Triton with TRT-LLM, we have written a custom wrapper for our LLM.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain.prompts</span> <span class="kn">import</span> <span class="n">PromptTemplate</span>

<span class="n">LLAMA_PROMPT_TEMPLATE</span> <span class="o">=</span> <span class="p">(</span>
 <span class="s2">&quot;&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;&quot;</span>
 <span class="s2">&quot;Use the following context to answer the user&#39;s question. If you don&#39;t know the answer, just say that you don&#39;t know, don&#39;t try to make up an answer.&quot;</span>
 <span class="s2">&quot;&lt;&lt;/SYS&gt;&gt;&quot;</span>
 <span class="s2">&quot;&lt;s&gt;[INST] Context: </span><span class="si">{context}</span><span class="s2"> Question: </span><span class="si">{question}</span><span class="s2"> Only return the helpful answer below and nothing else. Helpful answer:[/INST]&quot;</span>
<span class="p">)</span>

<span class="n">LLAMA_PROMPT</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="o">.</span><span class="n">from_template</span><span class="p">(</span><span class="n">LLAMA_PROMPT_TEMPLATE</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="step-3-load-documents-retrieval">
<h2>Step 3: Load Documents <a class="reference external" href="https://python.langchain.com/docs/modules/data_connection/"><em>(Retrieval)</em></a><a class="headerlink" href="#step-3-load-documents-retrieval" title="Permalink to this headline"></a></h2>
<p>LangChain provides a variety of <a class="reference external" href="https://python.langchain.com/docs/integrations/document_loaders">document loaders</a> that load various types of documents (HTML, PDF, code) from many different sources and locations (private s3 buckets, public websites).</p>
<p>Document loaders load data from a source as <strong>Documents</strong>. A <strong>Document</strong> is a piece of text (the page_content) and associated metadata. Document loaders provide a <code class="docutils literal notranslate"><span class="pre">load</span></code> method for loading data as documents from a configured source.</p>
<p>In this example, we use a LangChain <a class="reference external" href="https://python.langchain.com/docs/integrations/document_loaders/unstructured_file"><code class="docutils literal notranslate"><span class="pre">UnstructuredFileLoader</span></code></a> to load a research paper about Llama2 from Meta.</p>
<p><a class="reference external" href="https://python.langchain.com/docs/integrations/document_loaders">Here</a> are some of the other document loaders available from LangChain.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span><span class="w"> </span>wget<span class="w"> </span>-O<span class="w"> </span><span class="s2">&quot;llama2_paper.pdf&quot;</span><span class="w"> </span>-nc<span class="w"> </span>--user-agent<span class="o">=</span><span class="s2">&quot;Mozilla&quot;</span><span class="w"> </span>https://arxiv.org/pdf/2307.09288.pdf
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain.document_loaders</span> <span class="kn">import</span> <span class="n">UnstructuredFileLoader</span>
<span class="n">loader</span> <span class="o">=</span> <span class="n">UnstructuredFileLoader</span><span class="p">(</span><span class="s2">&quot;llama2_paper.pdf&quot;</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">load</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="step-4-transform-documents-retrieval">
<h2>Step 4: Transform Documents <a class="reference external" href="https://python.langchain.com/docs/modules/data_connection/"><em>(Retrieval)</em></a><a class="headerlink" href="#step-4-transform-documents-retrieval" title="Permalink to this headline"></a></h2>
<p>Once documents have been loaded, they are often transformed. One method of transformation is known as <strong>chunking</strong>, which breaks down large pieces of text, for example, a long document, into smaller segments. This technique is valuable because it helps <a class="reference external" href="https://www.pinecone.io/learn/chunking-strategies/">optimize the relevance of the content returned from the vector database</a>.</p>
<p>LangChain provides a <a class="reference external" href="https://python.langchain.com/docs/integrations/document_transformers/">variety of document transformers</a>, such as text splitters. In this example, we use a <a class="reference external" href="https://api.python.langchain.com/en/latest/text_splitter/langchain.text_splitter.SentenceTransformersTokenTextSplitter.html#langchain.text_splitter.SentenceTransformersTokenTextSplitter"><code class="docutils literal notranslate"><span class="pre">SentenceTransformersTokenTextSplitter</span></code></a>. The <code class="docutils literal notranslate"><span class="pre">SentenceTransformersTokenTextSplitter</span></code> is a specialized text splitter for use with the sentence-transformer models. The default behaviour is to split the text into chunks that fit the token window of the sentence transformer model that you would like to use. This sentence transformer model is used to generate the embeddings from documents.</p>
<p>There are some nuanced complexities to text splitting since semantically related text, in theory, should be kept together.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>
<span class="kn">from</span> <span class="nn">langchain.text_splitter</span> <span class="kn">import</span> <span class="n">SentenceTransformersTokenTextSplitter</span>
<span class="n">TEXT_SPLITTER_MODEL</span> <span class="o">=</span> <span class="s2">&quot;intfloat/e5-large-v2&quot;</span>
<span class="n">TEXT_SPLITTER_TOKENS_PER_CHUNK</span> <span class="o">=</span> <span class="mi">510</span>
<span class="n">TEXT_SPLITTER_CHUNCK_OVERLAP</span> <span class="o">=</span> <span class="mi">200</span>

<span class="n">text_splitter</span> <span class="o">=</span> <span class="n">SentenceTransformersTokenTextSplitter</span><span class="p">(</span>
    <span class="n">model_name</span><span class="o">=</span><span class="n">TEXT_SPLITTER_MODEL</span><span class="p">,</span>
    <span class="n">tokens_per_chunk</span><span class="o">=</span><span class="n">TEXT_SPLITTER_TOKENS_PER_CHUNK</span><span class="p">,</span>
    <span class="n">chunk_overlap</span><span class="o">=</span><span class="n">TEXT_SPLITTER_CHUNCK_OVERLAP</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">documents</span> <span class="o">=</span> <span class="n">text_splitter</span><span class="o">.</span><span class="n">split_documents</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;--- </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">start_time</span><span class="si">}</span><span class="s2"> seconds ---&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s view a sample of content that is chunked together in the documents.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">documents</span><span class="p">[</span><span class="mi">40</span><span class="p">]</span><span class="o">.</span><span class="n">page_content</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="step-5-generate-embeddings-and-store-embeddings-in-the-vector-store-retrieval">
<h2>Step 5: Generate Embeddings and Store Embeddings in the Vector Store <a class="reference external" href="https://python.langchain.com/docs/modules/data_connection/"><em>(Retrieval)</em></a><a class="headerlink" href="#step-5-generate-embeddings-and-store-embeddings-in-the-vector-store-retrieval" title="Permalink to this headline"></a></h2>
<section id="a-generate-embeddings">
<h3>a) Generate Embeddings<a class="headerlink" href="#a-generate-embeddings" title="Permalink to this headline"></a></h3>
<p><a class="reference external" href="https://python.langchain.com/docs/modules/data_connection/text_embedding/">Embeddings</a> for documents are created by vectorizing the document text; this vectorization captures the semantic meaning of the text. This allows you to quickly and efficiently find other pieces of text that are similar. The embedding model used below is <a class="reference external" href="https://huggingface.co/intfloat/e5-large-v2">intfloat/e5-large-v2</a>.</p>
<p>LangChain provides a wide variety of <a class="reference external" href="https://python.langchain.com/docs/integrations/text_embedding">embedding models</a> from many providers and makes it simple to swap out the models.</p>
<p>When a user sends in their query, the query is also embedded using the same embedding model that was used to embed the documents. As explained earlier, this allows to find similar (relevant) documents to the user’s query.</p>
</section>
<section id="b-store-document-embeddings-in-the-vector-store">
<h3>b) Store Document Embeddings in the Vector Store<a class="headerlink" href="#b-store-document-embeddings-in-the-vector-store" title="Permalink to this headline"></a></h3>
<p>Once the document embeddings are generated, they are stored in a vector store so that at query time we can:</p>
<ol class="arabic simple">
<li><p>Embed the user query and</p></li>
<li><p>Retrieve the embedding vectors that are most similar to the embedding query.</p></li>
</ol>
<p>A vector store takes care of storing the embedded data and performing a vector search.</p>
<p>LangChain provides support for a <a class="reference external" href="https://python.langchain.com/docs/integrations/vectorstores/">great selection of vector stores</a>.</p>
<div class="alert alert-block alert-info">
<p>⚠️ For this workflow, <a class="reference external" href="https://milvus.io/">Milvus</a> vector database is running as a microservice.</p>
</div><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain.embeddings</span> <span class="kn">import</span> <span class="n">HuggingFaceEmbeddings</span>
<span class="kn">from</span> <span class="nn">langchain.vectorstores</span> <span class="kn">import</span> <span class="n">Milvus</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="c1">#Running the model on CPU as we want to conserve gpu memory.</span>
<span class="c1">#In the production deployment (API server shown as part of the 5th notebook we run the model on GPU)</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;intfloat/e5-large-v2&quot;</span>
<span class="n">model_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;device&quot;</span><span class="p">:</span> <span class="s2">&quot;cpu&quot;</span><span class="p">}</span>
<span class="n">encode_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;normalize_embeddings&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">}</span>
<span class="n">hf_embeddings</span> <span class="o">=</span> <span class="n">HuggingFaceEmbeddings</span><span class="p">(</span>
    <span class="n">model_name</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span>
    <span class="n">model_kwargs</span><span class="o">=</span><span class="n">model_kwargs</span><span class="p">,</span>
    <span class="n">encode_kwargs</span><span class="o">=</span><span class="n">encode_kwargs</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">vectorstore</span> <span class="o">=</span> <span class="n">Milvus</span><span class="o">.</span><span class="n">from_documents</span><span class="p">(</span><span class="n">documents</span><span class="o">=</span><span class="n">documents</span><span class="p">,</span> <span class="n">embedding</span><span class="o">=</span><span class="n">hf_embeddings</span><span class="p">,</span> <span class="n">connection_args</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;host&quot;</span><span class="p">:</span> <span class="s2">&quot;milvus&quot;</span><span class="p">,</span> <span class="s2">&quot;port&quot;</span><span class="p">:</span> <span class="s2">&quot;19530&quot;</span><span class="p">})</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;--- </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">start_time</span><span class="si">}</span><span class="s2"> seconds ---&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Simple Example: Retrieve Documents from the Vector Database</span>
<span class="c1"># note: this is just for demonstration purposes of a similarity search</span>
<span class="n">question</span> <span class="o">=</span> <span class="s2">&quot;Can you talk about safety evaluation of llama2 chat?&quot;</span>
<span class="n">docs</span> <span class="o">=</span> <span class="n">vectorstore</span><span class="o">.</span><span class="n">similarity_search</span><span class="p">(</span><span class="n">question</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">docs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">page_content</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<blockquote>
<div></div></blockquote>
</section>
</section>
<section id="simple-example-retrieve-documents-from-the-vector-database-retrieval">
<h2>Simple Example: Retrieve Documents from the Vector Database <a class="reference external" href="https://python.langchain.com/docs/modules/data_connection/"><em>(Retrieval)</em></a><a class="headerlink" href="#simple-example-retrieve-documents-from-the-vector-database-retrieval" title="Permalink to this headline"></a></h2>
<p>Given a user query, relevant splits for the question are returned through a <strong>similarity search</strong>. This is also known as a semantic search, and it is done with meaning. It is different from a lexical search, where the search engine looks for literal matches of the query words or variants of them, without understanding the overall meaning of the query. A semantic search tends to generate more relevant results than a lexical search.
<img alt="vector_stores.jpeg" src="../_images/vector_stores.jpeg" /></p>
</section>
<section id="step-6-compose-a-streamed-answer-using-a-chain">
<h2>Step 6: Compose a streamed answer using a Chain<a class="headerlink" href="#step-6-compose-a-streamed-answer-using-a-chain" title="Permalink to this headline"></a></h2>
<p>We have already integrated the Llama2 TRT LLM with the help of LangChain connector, loaded and transformed documents, and generated and stored document embeddings in a vector database. To finish the pipeline, we need to add a few more LangChain components and combine all the components together with a <a class="reference external" href="https://python.langchain.com/docs/modules/chains/">chain</a>.</p>
<p>A <a class="reference external" href="https://python.langchain.com/docs/modules/chains/">LangChain chain</a> combines components together. In this case, we use  <a class="reference external" href="https://python.langchain.com/docs/expression_language/why">Langchain Expression Language</a> to build a chain.</p>
<p>We formulate the prompt placeholders (context and question) and pipe it to our trt-llm connector as shown below and finally stream the result.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain_core.runnables</span> <span class="kn">import</span> <span class="n">RunnablePassthrough</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="n">chain</span> <span class="o">=</span> <span class="p">(</span>
    <span class="p">{</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="n">vectorstore</span><span class="o">.</span><span class="n">as_retriever</span><span class="p">(),</span> <span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="n">RunnablePassthrough</span><span class="p">()}</span>
    <span class="o">|</span> <span class="n">LLAMA_PROMPT</span>
    <span class="o">|</span> <span class="n">llm</span>
<span class="p">)</span>
<span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">chain</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="n">question</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">--- </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">start_time</span><span class="si">}</span><span class="s2"> seconds ---&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="01-llm-streaming-client.html" class="btn btn-neutral float-left" title="LLM Streaming Client" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="03_llama_index_simple.html" class="btn btn-neutral float-right" title="Q&amp;A with LlamaIndex" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023-2024, NVIDIA.</p>
  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 



</body>
</html>