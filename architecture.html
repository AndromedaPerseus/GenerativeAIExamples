<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Architecture &mdash; NVIDIA Generative AI Examples 0.5.0 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" type="text/css" />
      <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="_static/omni-style.css" type="text/css" />
      <link rel="stylesheet" href="_static/custom.css" type="text/css" />
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/clipboard.min.js"></script>
        <script src="_static/copybutton.js"></script>
        <script src="_static/version.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="NeMo Framework Inference Server" href="llm-inference-server.html" />
    <link rel="prev" title="Multimodal Models from NVIDIA AI Endpoints with LangChain Agent" href="notebooks/09_Agent_use_tools_leveraging_NVIDIA_AI_endpoints.html" />
 


</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >


<a href="index.html">
  <img src="_static/nvidia-logo-white.png" class="logo" alt="Logo"/>
</a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">RAG Pipelines for Developers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html">About the RAG Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="support-matrix.html">Support Matrix</a></li>
<li class="toctree-l1"><a class="reference internal" href="ai-foundation-models.html">AI Foundation Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="local-gpu.html">Local GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi-gpu.html">Multi-GPU for Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="query-decomposition.html">Query Decomposition</a></li>
<li class="toctree-l1"><a class="reference internal" href="quantized-llm-model.html">Quantized Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="structured-data.html">Structured Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="multimodal-data.html">Multimodal Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi-turn.html">Multi-turn</a></li>
<li class="toctree-l1"><a class="reference internal" href="using-sample-web-application.html">Sample Chat Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="vector-database.html">Alternative Vector Database</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="evaluation.html">Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="observability.html">Observability</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Jupyter Notebooks</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="notebooks/00-llm-non-streaming-nemotron.html">Basics: Prompt, Client, and Responses</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/01-llm-streaming-client.html">LLM Streaming Client</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/02_langchain_simple.html">Q&amp;A with LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/03_llama_index_simple.html">Q&amp;A with LlamaIndex</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/04_llamaindex_hier_node_parser.html">Advanced Q&amp;A with LlamaIndex</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/05_dataloader.html">Press Release Chat Bot</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/07_Option%281%29_NVIDIA_AI_endpoint_simple.html">NVIDIA AI Endpoints with LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/07_Option%282%29_minimalistic_RAG_with_langchain_local_HF_LLM.html">LangChain with Local Llama 2 Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/08_Option%281%29_llama_index_with_NVIDIA_AI_endpoint.html">NVIDIA AI Endpoints, LlamaIndex, and LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/08_Option%282%29_llama_index_with_HF_local_LLM.html">HF Checkpoints with LlamaIndex and LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/09_Agent_use_tools_leveraging_NVIDIA_AI_endpoints.html">Multimodal Models from NVIDIA AI Endpoints with LangChain Agent</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Software Components</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="llm-inference-server.html">NeMo Framework Inference Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="frontend.html">RAG Playground Web Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="jupyter-server.html">Jupyter Notebook Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="chain-server.html">Chain Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="configuration.html">Software Component Configuration</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">NVIDIA Generative AI Examples</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Architecture</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <!--
  SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  SPDX-License-Identifier: Apache-2.0

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

  http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<section id="architecture">
<h1>Architecture<a class="headerlink" href="#architecture" title="Permalink to this headline"></a></h1>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#overview-of-software-components" id="id1">Overview of Software Components</a></p></li>
<li><p><a class="reference internal" href="#nvidia-ai-components" id="id2">NVIDIA AI Components</a></p>
<ul>
<li><p><a class="reference internal" href="#nvidia-tensorrt-llm-optimization" id="id3">NVIDIA TensorRT-LLM Optimization</a></p></li>
<li><p><a class="reference internal" href="#nvidia-nemo-framework-inference-container" id="id4">NVIDIA NeMo Framework Inference Container</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#inference-pipeline" id="id5">Inference Pipeline</a></p></li>
<li><p><a class="reference internal" href="#document-ingestion-and-retrieval" id="id6">Document Ingestion and Retrieval</a></p></li>
<li><p><a class="reference internal" href="#user-query-and-response-generation" id="id7">User Query and Response Generation</a></p></li>
<li><p><a class="reference internal" href="#llm-inference-server" id="id8">LLM Inference Server</a></p></li>
<li><p><a class="reference internal" href="#vector-db" id="id9">Vector DB</a></p></li>
</ul>
</div>
<section id="overview-of-software-components">
<h2>Overview of Software Components<a class="headerlink" href="#overview-of-software-components" title="Permalink to this headline"></a></h2>
<p>The default sample deployment contains:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://docs.nvidia.com/nemo-framework/user-guide/latest/index.html">NVIDIA NeMo Framework Inference Server</a> - part of NVIDIA AI Enterprise solution</p></li>
<li><p><a class="reference external" href="https://developer.nvidia.com/tensorrt">NVIDIA TensorRT-LLM</a> - for low latency and high throughput inference for LLMs</p></li>
<li><p><a class="reference external" href="https://github.com/langchain-ai/langchain/">LangChain</a> and <a class="reference external" href="https://www.llamaindex.ai/">LlamaIndex</a> for combining language model components and easily constructing question-answering from a company’s database</p></li>
<li><p><a class="reference internal" href="jupyter-server.html"><span class="doc std std-doc">Sample Jupyter Notebooks</span></a> and <a class="reference internal" href="frontend.html"><span class="doc std std-doc">chat bot web application/API calls</span></a> so that you can test the chat system in an interactive manner</p></li>
<li><p><a class="reference external" href="https://milvus.io/docs/install_standalone-docker.md">Milvus</a> - Generated embeddings are stored in a vector database. The vector DB used in this workflow is Milvus. Milvus is an open-source vector database capable of NVIDIA GPU-accelerated vector searches.</p></li>
<li><p><a class="reference external" href="https://huggingface.co/embaas/sentence-transformers-e5-large-v2">e5-large-v2 model</a> from Hugging Face to generate the embeddings.</p></li>
<li><p><a class="reference external" href="https://github.com/facebookresearch/llama/">Llama2</a>, an open source model from Meta, to formulate natural responses.</p></li>
</ul>
<p>This sample deployment is a reference for you to build your own enterprise AI solution with minimal effort.
The software components are used to deploy models and inference pipeline, integrated together with the additional components as indicated in the following diagram:</p>
<p><img alt="Diagram" src="_images/image0.png" /></p>
</section>
<section id="nvidia-ai-components">
<h2>NVIDIA AI Components<a class="headerlink" href="#nvidia-ai-components" title="Permalink to this headline"></a></h2>
<p>The sample deployment uses a variety of NVIDIA AI components to customize and deploy the RAG-based chat bot example.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM">NVIDIA TensorRT-LLM</a></p></li>
<li><p><a class="reference external" href="https://developer.nvidia.com/nemo">NVIDIA NeMo Inference Container</a></p></li>
</ul>
<section id="nvidia-tensorrt-llm-optimization">
<h3>NVIDIA TensorRT-LLM Optimization<a class="headerlink" href="#nvidia-tensorrt-llm-optimization" title="Permalink to this headline"></a></h3>
<p>An LLM can be optimized using TensorRT-LLM. NVIDIA NeMo uses TensorRT for LLMs (TensorRT-LLM), for deployment which accelerates and maximizes inference performance on the latest LLMs.
The sample deployment leverages a Llama 2 (13B parameters) chat model.
The foundational model is converted to TensorRT format using TensorRT-LLM for optimized inference.</p>
</section>
<section id="nvidia-nemo-framework-inference-container">
<h3>NVIDIA NeMo Framework Inference Container<a class="headerlink" href="#nvidia-nemo-framework-inference-container" title="Permalink to this headline"></a></h3>
<p>With NeMo Framework Inference Container, the optimized LLM can be deployed for high-performance, cost-effective, and low-latency inference. NeMo Framework Inference Container contains modules and scripts to help exporting LLM models to <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM">TensorRT-LLM</a> and deploying them to <a class="reference external" href="https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/index.html">Triton Inference Server</a> with easy-to-use APIs.</p>
</section>
</section>
<section id="inference-pipeline">
<h2>Inference Pipeline<a class="headerlink" href="#inference-pipeline" title="Permalink to this headline"></a></h2>
<p>To get started with the inferencing pipeline, we connect the customized LLM to a sample proprietary data source.
This knowledge can come in many forms: product specifications, HR documents, or finance spreadsheets.
Enhancing the model’s capabilities with this knowledge can be done with RAG.</p>
<p>Because foundational LLMs are not trained on your proprietary enterprise data and are only trained up to a fixed point in time, they need to be augmented with additional data.
RAG consists of two processes.
First, <em>retrieval</em> of data from document repositories, databases, or APIs that are all outside of the foundational model’s knowledge.
Second, <em>generation</em> of responses via inference.
The following graphic describes an overview of this inference pipeline:</p>
<p><img alt="Diagram" src="_images/image1.png" /></p>
</section>
<section id="document-ingestion-and-retrieval">
<h2>Document Ingestion and Retrieval<a class="headerlink" href="#document-ingestion-and-retrieval" title="Permalink to this headline"></a></h2>
<p>RAG begins with a knowledge base of relevant up-to-date information.
Because data within an enterprise is frequently updated, the ingestion of documents into a knowledge base is a recurring process and could be scheduled as a job.
Next, content from the knowledge base is passed to an embedding model such as e5-large-v2 that the sample deployment uses.
The embedding model converts the content to vectors, referred to as <em>embeddings</em>.
Generating embeddings is a critical step in RAG.
The embeddings provide dense numerical representations of textual information.
These embeddings are stored in a vector database, in this case Milvus, which is <a class="reference external" href="https://developer.nvidia.com/blog/accelerating-vector-search-using-gpu-powered-indexes-with-rapids-raft">RAFT accelerated</a>.</p>
</section>
<section id="user-query-and-response-generation">
<h2>User Query and Response Generation<a class="headerlink" href="#user-query-and-response-generation" title="Permalink to this headline"></a></h2>
<p>When a user query is sent to the inference server, it is converted to an embedding using the embedding model.
This is the same embedding model that is used to convert the documents in the knowledge base, e5-large-v2, in the case of this sample deployment.
The database performs a similarity/semantic search to find the vectors that most closely resemble the user’s intent and provides them to the LLM as enhanced context.
Because Milvus is RAFT accelerated, the similarity serach is optimized on the GPU.
Lastly, the LLM generates a full answer that is streamed to the user.
This is all done with ease using <a class="reference external" href="https://github.com/langchain-ai/langchain/">LangChain</a> and <a class="reference external" href="https://www.llamaindex.ai">LlamaIndex</a>.</p>
<p>The following diagram illustrates the ingestion of documents and generation of responses.</p>
<p><img alt="Diagram" src="_images/image2.png" /></p>
<p>LangChain enables you to write LLM wrappers for your own custom LLMs.
NVIDIA provides a sample wrapper for streaming responses from a TensorRT-LLM Llama 2 model running on Triton Inference Server.
This wrapper enables us to leverage LangChain’s standard interface for interacting with LLMs while still achieving vast performance speedup from TensorRT-LLM and scalable and flexible inference from Triton Inference Server.</p>
<p>A sample chat bot web application is provided in the sample deployment so that you can test the chat system in an interactive manner.
Requests to the chat system are wrapped in API calls, so these can be abstracted to other applications.</p>
<p>An additional method of customization in the inference pipeline is possible with a prompt template.
A prompt template is a pre-defined recipe for generating prompts for language models.
The prompts can contain instructions, few-shot examples, and context that is appropriate for a given task.
In our sample deployment, we prompt our model to generate safe and polite responses.</p>
</section>
<section id="llm-inference-server">
<h2>LLM Inference Server<a class="headerlink" href="#llm-inference-server" title="Permalink to this headline"></a></h2>
<p>The LLM Inference Server uses models that are stored in a model repository.
This repository is available locally to serve inference requests.
After they are available in Triton Inference Server, inference requests are sent from a client application.
Python and C++ libraries provide APIs to simplify communication.
Clients send HTTP/REST requests directly to Triton Inference Server using HTTP/REST or gRPC protocols.</p>
<p>Within the sample deployment, the Llama2 LLM was optimized using NVIDIA TensorRT for LLMs (TRT-LLM).
This software accelerates and maximizes inference performance on the latest LLMs.</p>
</section>
<section id="vector-db">
<h2>Vector DB<a class="headerlink" href="#vector-db" title="Permalink to this headline"></a></h2>
<p>Milvus is an open-source vector database built to power embedding similarity search and AI applications.
The database makes unstructured data from API calls, PDFs, and other documents more accessible by storing them as embeddings.</p>
<p>When content from the knowledge base is passed to an embedding model, e5-large-v2, the model converts the content to vectors–referred to as <em>embeddings</em>.
These embeddings are stored in the vector database.
The sample deployment uses Milvus as the vector database.
Milvus is an open-source vector database capable of NVIDIA GPU-accelerated vector searches.</p>
<p>If needed, see Milvus’s <a class="reference external" href="https://milvus.io/docs/install_standalone-docker.md/">documentation</a> for how to configure a Docker Compose file for Milvus.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="notebooks/09_Agent_use_tools_leveraging_NVIDIA_AI_endpoints.html" class="btn btn-neutral float-left" title="Multimodal Models from NVIDIA AI Endpoints with LangChain Agent" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="llm-inference-server.html" class="btn btn-neutral float-right" title="NeMo Framework Inference Server" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023-2024, NVIDIA.</p>
  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 



</body>
</html>