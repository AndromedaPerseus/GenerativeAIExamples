<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Quantized LLM Inference Model &mdash; NVIDIA Generative AI Examples 0.5.0 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" type="text/css" />
      <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="_static/omni-style.css" type="text/css" />
      <link rel="stylesheet" href="_static/custom.css" type="text/css" />
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/clipboard.min.js"></script>
        <script src="_static/copybutton.js"></script>
        <script src="_static/version.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Structured Data" href="structured-data.html" />
    <link rel="prev" title="Query Decomposition" href="query-decomposition.html" />
 


</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >


<a href="index.html">
  <img src="_static/nvidia-logo-white.png" class="logo" alt="Logo"/>
</a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">RAG Pipelines for Developers</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="index.html">About the RAG Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="support-matrix.html">Support Matrix</a></li>
<li class="toctree-l1"><a class="reference internal" href="api-catalog.html">API Catalog Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="local-gpu.html">Local GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi-gpu.html">Multi-GPU for Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="query-decomposition.html">Query Decomposition</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Quantized Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="structured-data.html">Structured Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="multimodal-data.html">Multimodal Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi-turn.html">Multi-turn</a></li>
<li class="toctree-l1"><a class="reference internal" href="using-sample-web-application.html">Sample Chat Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="vector-database.html">Alternative Vector Database</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="evaluation.html">Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="observability.html">Observability</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Jupyter Notebooks</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="notebooks/00-llm-non-streaming-nemotron.html">Basics: Prompt, Client, and Responses</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/01-llm-streaming-client.html">LLM Streaming Client</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/02_langchain_simple.html">Q&amp;A with LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/03_llama_index_simple.html">Q&amp;A with LlamaIndex</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/04_llamaindex_hier_node_parser.html">Advanced Q&amp;A with LlamaIndex</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/05_dataloader.html">Press Release Chat Bot</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/07_Option%281%29_NVIDIA_AI_endpoint_simple.html">NVIDIA AI Endpoints with LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/07_Option%282%29_minimalistic_RAG_with_langchain_local_HF_LLM.html">LangChain with Local Llama 2 Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/08_Option%281%29_llama_index_with_NVIDIA_AI_endpoint.html">NVIDIA AI Endpoints, LlamaIndex, and LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/08_Option%282%29_llama_index_with_HF_local_LLM.html">HF Checkpoints with LlamaIndex and LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/09_Agent_use_tools_leveraging_NVIDIA_AI_endpoints.html">Multimodal Models from NVIDIA AI Endpoints with LangChain Agent</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/10_RAG_for_HTML_docs_with_Langchain_NVIDIA_AI_Endpoints.html">Build a RAG chain by generating embeddings for NVIDIA Triton documentation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Software Components</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="architecture.html">Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="llm-inference-server.html">NeMo Framework Inference Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="frontend.html">RAG Playground Web Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="jupyter-server.html">Jupyter Notebook Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="chain-server.html">Chain Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="configuration.html">Software Component Configuration</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">NVIDIA Generative AI Examples</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Quantized LLM Inference Model</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <!--
  SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  SPDX-License-Identifier: Apache-2.0

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

  http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<section id="quantized-llm-inference-model">
<h1>Quantized LLM Inference Model<a class="headerlink" href="#quantized-llm-inference-model" title="Permalink to this headline"></a></h1>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#example-features" id="id1">Example Features</a></p></li>
<li><p><a class="reference internal" href="#prerequisites" id="id2">Prerequisites</a></p></li>
<li><p><a class="reference internal" href="#download-the-llama-2-model-and-weights" id="id3">Download the Llama 2 Model and Weights</a></p></li>
<li><p><a class="reference internal" href="#download-tensorrt-llm-and-quantize-the-model" id="id4">Download TensorRT-LLM and Quantize the Model</a></p></li>
<li><p><a class="reference internal" href="#build-and-start-the-containers" id="id5">Build and Start the Containers</a></p></li>
<li><p><a class="reference internal" href="#stopping-the-containers" id="id6">Stopping the Containers</a></p></li>
<li><p><a class="reference internal" href="#next-steps" id="id7">Next Steps</a></p></li>
</ul>
</div>
<section id="example-features">
<h2>Example Features<a class="headerlink" href="#example-features" title="Permalink to this headline"></a></h2>
<p>This example deploys a developer RAG pipeline for chat Q&amp;A and serves inferencing with the NeMo Framework Inference container across multiple local GPUs with a
quantized version of the Llama 7B chat model.</p>
<p>This example uses a local host with an NVIDIA A100, H100, or L40S GPU.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 11%" />
<col style="width: 11%" />
<col style="width: 11%" />
<col style="width: 11%" />
<col style="width: 11%" />
<col style="width: 11%" />
<col style="width: 11%" />
<col style="width: 11%" />
<col style="width: 11%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>Embedding</p></th>
<th class="head"><p>Framework</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Multi-GPU</p></th>
<th class="head"><p>TRT-LLM</p></th>
<th class="head"><p>Model Location</p></th>
<th class="head"><p>Triton</p></th>
<th class="head"><p>Vector Database</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>llama-2-7b-chat</p></td>
<td><p>e5-large-v2</p></td>
<td><p>LlamaIndex</p></td>
<td><p>QA chatbot</p></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
<td><p>Local Model</p></td>
<td><p>YES</p></td>
<td><p>Milvus</p></td>
</tr>
</tbody>
</table>
</section>
<section id="prerequisites">
<h2>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this headline"></a></h2>
<ul>
<li><p>Clone the Generative AI examples Git repository using Git LFS:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sudo<span class="w"> </span>apt<span class="w"> </span>-y<span class="w"> </span>install<span class="w"> </span>git-lfs
<span class="gp">$ </span>git<span class="w"> </span>clone<span class="w"> </span>git@github.com:NVIDIA/GenerativeAIExamples.git
<span class="gp">$ </span><span class="nb">cd</span><span class="w"> </span>GenerativeAIExamples/
<span class="gp">$ </span>git<span class="w"> </span>lfs<span class="w"> </span>pull
</pre></div>
</div>
</li>
<li><p>A host with one or more NVIDIA A100, H100, or L40S GPU.</p></li>
<li><p>Verify NVIDIA GPU driver version 535 or later is installed and that the GPU is in compute mode:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>nvidia-smi<span class="w"> </span>-q<span class="w"> </span>-d<span class="w"> </span>compute
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">==============NVSMI LOG==============</span>

<span class="go">Timestamp                                 : Sun Nov 26 21:17:25 2023</span>
<span class="hll"><span class="go">Driver Version                            : 535.129.03</span>
</span><span class="go">CUDA Version                              : 12.2</span>

<span class="go">Attached GPUs                             : 2</span>
<span class="go">GPU 00000000:CA:00.0</span>
<span class="hll"><span class="go">    Compute Mode                          : Default</span>
</span>
<span class="go">GPU 00000000:FA:00.0</span>
<span class="go">    Compute Mode                          : Default</span>
</pre></div>
</div>
<p>If the driver is not installed or below version 535, refer to the <a class="reference external" href="https://docs.nvidia.com/datacenter/tesla/tesla-installation-notes/index.html"><em>NVIDIA Driver Installation Quickstart Guide</em></a>.</p>
</li>
<li><p>Install Docker Engine and Docker Compose.
Refer to the instructions for <a class="reference external" href="https://docs.docker.com/engine/install/ubuntu/">Ubuntu</a>.</p></li>
<li><p>Install the NVIDIA Container Toolkit.</p>
<ol class="arabic">
<li><p>Refer to the <a class="reference external" href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html">installation documentation</a>.</p></li>
<li><p>When you configure the runtime, set the NVIDIA runtime as the default:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sudo<span class="w"> </span>nvidia-ctk<span class="w"> </span>runtime<span class="w"> </span>configure<span class="w"> </span>--runtime<span class="o">=</span>docker<span class="w"> </span>--set-as-default
</pre></div>
</div>
<p>If you did not set the runtime as the default, you can reconfigure the runtime by running the preceding command.</p>
</li>
<li><p>Verify the NVIDIA container toolkit is installed and configured as the default container runtime:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>cat<span class="w"> </span>/etc/docker/daemon.json
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;default-runtime&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;nvidia&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;runtimes&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;nvidia&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;args&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[],</span>
<span class="w">            </span><span class="nt">&quot;path&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;nvidia-container-runtime&quot;</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
<li><p>Run the <code class="docutils literal notranslate"><span class="pre">nvidia-smi</span></code> command in a container to verify the configuration:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sudo<span class="w"> </span>docker<span class="w"> </span>run<span class="w"> </span>--rm<span class="w"> </span>--runtime<span class="o">=</span>nvidia<span class="w"> </span>--gpus<span class="w"> </span>all<span class="w"> </span>ubuntu<span class="w"> </span>nvidia-smi<span class="w"> </span>-L
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">GPU 0: NVIDIA A100 80GB PCIe (UUID: GPU-d8ce95c1-12f7-3174-6395-e573163a2ace)</span>
<span class="go">GPU 1: NVIDIA A100 80GB PCIe (UUID: GPU-1d37ef30-0861-de64-a06d-73257e247a0d)</span>
</pre></div>
</div>
</li>
</ol>
</li>
<li><p>Optional: Enable NVIDIA Riva automatic speech recognition (ASR) and text to speech (TTS).</p>
<ul>
<li><p>To launch a Riva server locally, refer to the <a class="reference external" href="https://docs.nvidia.com/deeplearning/riva/user-guide/docs/quick-start-guide.html">Riva Quick Start Guide</a>.</p>
<ul class="simple">
<li><p>In the provided <code class="docutils literal notranslate"><span class="pre">config.sh</span></code> script, set <code class="docutils literal notranslate"><span class="pre">service_enabled_asr=true</span></code> and <code class="docutils literal notranslate"><span class="pre">service_enabled_tts=true</span></code>, and select the desired ASR and TTS languages by adding the appropriate language codes to <code class="docutils literal notranslate"><span class="pre">asr_language_code</span></code> and <code class="docutils literal notranslate"><span class="pre">tts_language_code</span></code>.</p></li>
<li><p>After the server is running, assign its IP address (or hostname) and port (50051 by default) to <code class="docutils literal notranslate"><span class="pre">RIVA_API_URI</span></code> in <code class="docutils literal notranslate"><span class="pre">deploy/compose/compose.env</span></code>.</p></li>
</ul>
</li>
<li><p>Alternatively, you can use a hosted Riva API endpoint. You might need to obtain an API key and/or Function ID for access.</p>
<p>In <code class="docutils literal notranslate"><span class="pre">deploy/compose/compose.env</span></code>, make the following assignments as necessary:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">RIVA_API_URI</span><span class="o">=</span><span class="s2">&quot;&lt;riva-api-address/hostname&gt;:&lt;port&gt;&quot;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">RIVA_API_KEY</span><span class="o">=</span><span class="s2">&quot;&lt;riva-api-key&gt;&quot;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">RIVA_FUNCTION_ID</span><span class="o">=</span><span class="s2">&quot;&lt;riva-function-id&gt;&quot;</span>
</pre></div>
</div>
</li>
</ul>
</li>
</ul>
</section>
<section id="download-the-llama-2-model-and-weights">
<h2>Download the Llama 2 Model and Weights<a class="headerlink" href="#download-the-llama-2-model-and-weights" title="Permalink to this headline"></a></h2>
<ol class="arabic">
<li><p>Go to <a class="reference external" href="https://huggingface.co/models">https://huggingface.co/models</a>.</p>
<ul class="simple">
<li><p>Locate the model to download, such as <a class="reference external" href="https://huggingface.co/meta-llama/Llama-2-7b-chat-hf">Llama 2 7B chat HF</a>.</p></li>
<li><p>Follow the information about accepting the license terms from Meta.</p></li>
<li><p>Log in or sign up for an account with Hugging Face.</p></li>
</ul>
</li>
<li><p>After you are granted access, clone the repository by clicking the vertical ellipses button and selecting <strong>Clone repository</strong>.</p>
<p>During the clone, you might be asked for your username and password multiple times.
Provide the information until the clone is complete.</p>
</li>
</ol>
</section>
<section id="download-tensorrt-llm-and-quantize-the-model">
<h2>Download TensorRT-LLM and Quantize the Model<a class="headerlink" href="#download-tensorrt-llm-and-quantize-the-model" title="Permalink to this headline"></a></h2>
<p>The following steps summarize downloading the TensorRT-LLM repository,
building a container image, and quantizing the model.</p>
<ol class="arabic">
<li><p>Clone the NVIDIA TensorRT-LLM repository:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/NVIDIA/TensorRT-LLM.git
<span class="gp">$ </span><span class="nb">cd</span><span class="w"> </span>TensorRT-LLM
<span class="gp">$ </span>git<span class="w"> </span>checkout<span class="w"> </span>release/0.5.0
<span class="gp">$ </span>git<span class="w"> </span>submodule<span class="w"> </span>update<span class="w"> </span>--init<span class="w"> </span>--recursive
<span class="gp">$ </span>git<span class="w"> </span>lfs<span class="w"> </span>install
<span class="gp">$ </span>git<span class="w"> </span>lfs<span class="w"> </span>pull
</pre></div>
</div>
</li>
<li><p>Build the TensorRT-LLM Docker image:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>make<span class="w"> </span>-C<span class="w"> </span>docker<span class="w"> </span>release_build
</pre></div>
</div>
<p>Building the image can require more than 30 minutes and requires approximately 30 GB.
The image is named tensorrt_llm/release:latest.</p>
</li>
<li><p>Start the container.
Ensure that the container has one volume mount to the model directory and one volume mount to the TensorRT-LLM repository:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>run<span class="w"> </span>--rm<span class="w"> </span>-it<span class="w"> </span>--gpus<span class="w"> </span>all<span class="w"> </span>--ipc<span class="o">=</span>host<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span>&lt;path-to-llama-2-7b-chat-model&gt;:/model-store<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>:/repo<span class="w"> </span>-w<span class="w"> </span>/repo<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--ulimit<span class="w"> </span><span class="nv">memlock</span><span class="o">=</span>-1<span class="w"> </span>--shm-size<span class="o">=</span>20g<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>tensorrt_llm/release:latest<span class="w"> </span>bash
</pre></div>
</div>
</li>
<li><p>Install NVIDIA AMMO Toolkit in the container:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp"># </span>Obtain<span class="w"> </span>the<span class="w"> </span>cuda<span class="w"> </span>version<span class="w"> </span>from<span class="w"> </span>the<span class="w"> </span>system.<span class="w"> </span>Assuming<span class="w"> </span>nvcc<span class="w"> </span>is<span class="w"> </span>available<span class="w"> </span><span class="k">in</span><span class="w"> </span>path.
<span class="gp">$ </span><span class="nv">cuda_version</span><span class="o">=</span><span class="k">$(</span>nvcc<span class="w"> </span>--version<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span><span class="s1">&#39;release&#39;</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>awk<span class="w"> </span><span class="s1">&#39;{print $6}&#39;</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>awk<span class="w"> </span>-F<span class="s1">&#39;[V.]&#39;</span><span class="w"> </span><span class="s1">&#39;{print $2$3}&#39;</span><span class="k">)</span>
<span class="gp"># </span>Obtain<span class="w"> </span>the<span class="w"> </span>python<span class="w"> </span>version<span class="w"> </span>from<span class="w"> </span>the<span class="w"> </span>system.
<span class="gp">$ </span><span class="nv">python_version</span><span class="o">=</span><span class="k">$(</span>python3<span class="w"> </span>--version<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>awk<span class="w"> </span><span class="s1">&#39;{print $2}&#39;</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>awk<span class="w"> </span>-F.<span class="w"> </span><span class="s1">&#39;{print $1$2}&#39;</span><span class="k">)</span>
<span class="gp"># </span>Download<span class="w"> </span>and<span class="w"> </span>install<span class="w"> </span>the<span class="w"> </span>AMMO<span class="w"> </span>package<span class="w"> </span>from<span class="w"> </span>the<span class="w"> </span>DevZone.
<span class="gp">$ </span>wget<span class="w"> </span>https://developer.nvidia.com/downloads/assets/cuda/files/nvidia-ammo/nvidia_ammo-0.3.0.tar.gz
<span class="gp">$ </span>tar<span class="w"> </span>-xzf<span class="w"> </span>nvidia_ammo-0.3.0.tar.gz
<span class="gp">$ </span>pip<span class="w"> </span>install<span class="w"> </span>nvidia_ammo-0.3.0/nvidia_ammo-0.3.0+cu<span class="nv">$cuda_version</span>-cp<span class="nv">$python_version</span>-cp<span class="nv">$python_version</span>-linux_x86_64.whl
<span class="gp"># </span>Install<span class="w"> </span>the<span class="w"> </span>additional<span class="w"> </span>requirements
<span class="gp">$ </span>pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>examples/quantization/requirements.txt
</pre></div>
</div>
</li>
<li><p>Install version <code class="docutils literal notranslate"><span class="pre">0.25.0</span></code> of the accelerate Python package:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>pip<span class="w"> </span>install<span class="w"> </span><span class="nv">accelerate</span><span class="o">==</span><span class="m">0</span>.25.0
</pre></div>
</div>
</li>
<li><p>Run the quantization with the container:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>python3<span class="w"> </span>examples/llama/quantize.py<span class="w"> </span>--model_dir<span class="w"> </span>/model-store<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dtype<span class="w"> </span>float16<span class="w"> </span>--qformat<span class="w"> </span>int4_awq<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--export_path<span class="w"> </span>./llama-2-7b-4bit-gs128-awq.pt<span class="w"> </span>--calib_size<span class="w"> </span><span class="m">32</span>
</pre></div>
</div>
<p>Quantization can require more than 15 minutes to complete.
The sample command creates a <code class="docutils literal notranslate"><span class="pre">llama-2-7b-4bit-gs128-awq.pt</span></code>
quantized checkpoint.</p>
</li>
<li><p>Copy the quantized checkpoint directory to the model directory:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>cp<span class="w"> </span>&lt;quantized-checkpoint&gt;.pt<span class="w"> </span>&lt;model-dir&gt;
</pre></div>
</div>
</li>
</ol>
<p>The preceding steps summarize several documents from the NVIDIA TensorRT-LLM GitHub repository.
Refer to the repository for more detail about the following topics:</p>
<ul class="simple">
<li><p>Building the TensorRT-LLM image, refer to the <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/blob/release/0.5.0/docs/source/installation.md">installation.md</a> file in the release/0.5.0 branch.</p></li>
<li><p>Installing NVIDIA AMMO Toolkit, refer to the <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/blob/release/0.5.0/examples/quantization/README.md">README</a> file in the <code class="docutils literal notranslate"><span class="pre">examples/quantization</span></code> directory.</p></li>
<li><p>Running the <code class="docutils literal notranslate"><span class="pre">quantize.py</span></code> command, refer to <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/blob/release/0.5.0/examples/llama/README.md#awq">AWQ</a> in the <code class="docutils literal notranslate"><span class="pre">examples/llama</span></code> directory.</p></li>
</ul>
</section>
<section id="build-and-start-the-containers">
<h2>Build and Start the Containers<a class="headerlink" href="#build-and-start-the-containers" title="Permalink to this headline"></a></h2>
<ol class="arabic">
<li><p>In the Generative AI Examples repository, edit the <code class="docutils literal notranslate"><span class="pre">deploy/compose/compose.env</span></code> file.</p>
<ul>
<li><p>Update the <code class="docutils literal notranslate"><span class="pre">MODEL_DIRECTORY</span></code> variable to identify the Llama 2 model directory that contains the quantized checkpoint.</p></li>
<li><p>Uncomment the <code class="docutils literal notranslate"><span class="pre">QUANTIZATION</span></code> variable:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>export QUANTIZATION=&quot;int4_awq&quot;
</pre></div>
</div>
</li>
</ul>
</li>
<li><p>From the root of the repository, build the containers:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>compose<span class="w"> </span>--env-file<span class="w"> </span>deploy/compose/compose.env<span class="w"> </span>-f<span class="w"> </span>deploy/compose/rag-app-text-chatbot.yaml<span class="w"> </span>build
</pre></div>
</div>
</li>
<li><p>Start the containers:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>compose<span class="w"> </span>--env-file<span class="w"> </span>deploy/compose/compose.env<span class="w"> </span>-f<span class="w"> </span>deploy/compose/rag-app-text-chatbot.yaml<span class="w"> </span>up<span class="w"> </span>-d
</pre></div>
</div>
<p>NVIDIA Triton Inference Server can require 5 minutes to start. The <code class="docutils literal notranslate"><span class="pre">-d</span></code> flag starts the services in the background.</p>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">✔ Network nvidia-rag              Created</span>
<span class="go">✔ Container llm-inference-server  Started</span>
<span class="go">✔ Container notebook-server       Started</span>
<span class="go">✔ Container chain-server          Started</span>
<span class="go">✔ Container rag-playground        Started</span>
</pre></div>
</div>
</li>
<li><p>Start the Milvus vector database:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>compose<span class="w"> </span>--env-file<span class="w"> </span>deploy/compose/compose.env<span class="w"> </span>-f<span class="w"> </span>deploy/compose/docker-compose-vectordb.yaml<span class="w"> </span>up<span class="w"> </span>-d<span class="w"> </span>milvus
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">✔ Container milvus-minio       Started</span>
<span class="go">✔ Container milvus-etcd        Started</span>
<span class="go">✔ Container milvus-standalone  Started</span>
</pre></div>
</div>
</li>
<li><p>Confirm the containers are running:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>ps<span class="w"> </span>--format<span class="w"> </span><span class="s2">&quot;table {{.ID}}\t{{.Names}}\t{{.Status}}&quot;</span>
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">CONTAINER ID   NAMES                  STATUS</span>
<span class="go">256da0ecdb7b   rag-playground         Up 48 minutes</span>
<span class="go">2974aa4fb2ce   chain-server           Up 48 minutes</span>
<span class="go">4a8c4aebe4ad   notebook-server        Up 48 minutes</span>
<span class="go">5be2b57bb5c1   milvus-standalone      Up 48 minutes (healthy)</span>
<span class="go">ecf674c8139c   llm-inference-server   Up 48 minutes (healthy)</span>
<span class="go">a6609c22c171   milvus-minio           Up 48 minutes (healthy)</span>
<span class="go">b23c0858c4d4   milvus-etcd            Up 48 minutes (healthy)</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="stopping-the-containers">
<h2>Stopping the Containers<a class="headerlink" href="#stopping-the-containers" title="Permalink to this headline"></a></h2>
<ul>
<li><p>To uninstall, stop and remove the running containers from the root of the Generative AI Examples repository:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>compose<span class="w"> </span>--env-file<span class="w"> </span>deploy/compose/compose.env<span class="w"> </span>-f<span class="w"> </span>deploy/compose/rag-app-text-chatbot.yaml<span class="w"> </span>down
<span class="gp">$ </span>docker<span class="w"> </span>compose<span class="w"> </span>-f<span class="w"> </span>deploy/compose/docker-compose-vectordb.yaml<span class="w"> </span>down
</pre></div>
</div>
</li>
</ul>
</section>
<section id="next-steps">
<h2>Next Steps<a class="headerlink" href="#next-steps" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>Use the <a class="reference internal" href="using-sample-web-application.html"><span class="doc std std-doc">Using the Sample Chat Web Application</span></a>.</p></li>
<li><p><a class="reference internal" href="vector-database.html"><span class="doc std std-doc">Configuring an Alternative Vector Database</span></a></p></li>
<li><p>Run the sample Jupyter notebooks to learn about optional features.</p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="query-decomposition.html" class="btn btn-neutral float-left" title="Query Decomposition" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="structured-data.html" class="btn btn-neutral float-right" title="Structured Data" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023-2024, NVIDIA.</p>
  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 



</body>
</html>